var tipuesearch = {"pages":[{"title":"Starship: a terminal prompt that puts the most important info up front","text":"As part of my continuing quest to make my terminal more colourful , I've starting using Starship . Starship gives you a colourful, customisable prompt, with lots of useful information about your current programming environment, git branch, or shell built in. Installation The best way to install Starship is to follow the official instructions - usually there's a single terminal command to install, plus another to set it up for your specific shell. You'll need a \"nerd font\" that has some extra symbols in it - find lots of options here . Configuration Starship is configured through a configuration file at ~/.config/starship.toml . To get started, you can try one of the nice looking presets like Tokyo Night, which can be set up automatically with a single command: starship preset tokyo-night -o ~/.config/starship.toml See Starship's documentation for information about how to tweak the prompt for specific languages and tools.","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/starship-a-terminal-prompt-that-puts-the-most-important-info-up-front.html","loc":"https://sydney-informatics-hub.github.io/tidbits/starship-a-terminal-prompt-that-puts-the-most-important-info-up-front.html"},{"title":"CLI transfer to OneDrive","text":"Since the death of Cloudstor (RIP), we've had to rely on OneDrive to share data with external collaborators who cannot gain access to USyd supported compute platforms. When working with large datasets, up/downloading data to OneDrive requires a CLI solution that can be automated and sped up. While OneDrive has its own user interface for accessing the OneDrive API from the command-line its functionality is limited. Rclone is also capable of interacting with the OneDrive API, support other cloud services beyond OneDrive, and offers advanced features like synchronising directories and data verification using checksums. Configuration when you don't have sudo permissions (i.e. on Artemis/RDS) NOTE: on Artemis/rds you will not be able to open the provided link. To avoid this you'll have to configure rclone on your local machine. So ensure it is installed on a machine with external network access before you start this process. See section below for installation instructions. If on Artemis/RDS, load the module: module load rclone / 1.62 . 2 Then start an interactive session to configure your installation and follow the steps below to configure for OneDrive: rclone config Select new remote (n) Name your remote (e.g. Georgie-OneDrive) Select 'onedrive' as storage type Leave client ID and secret empty (hit enter a few times) Do not edit advanced config (n), use auto config (y) When asked whether or not to use a web bowser to authenticate rclone, select no (n) On your local machine (requires rclone is installed) run rclone authorize \"onedrive\" Copy the provided secret token and paste it in Artemis/rds terminal Select 'onedrive' as config type Specify which drive to use Accept configuration (y) Quit config (q) Configuration when you do have sudo permissions Start by installing and configuring rclone if on a VM or local machine: sudo apt install rclone Then start an interactive session to configure your installation and follow the steps below to configure for OneDrive: rclone config Select new remote (n) Name your remote (e.g. Georgie-OneDrive) Select 'onedrive' as storage type Leave client ID and secret empty (hit enter a few times) Do not edit advanced config (n), use auto config (y) Open link in your browser, accept permission request Back on CLI, select onedrive Specify which drive to use Accept configuration (y) Quit config (q) Transfer data Once configuration is set you can view your rclone configuration with: rclone config show To transfer data to your configured OneDrive account, simply run: rclone copy <source>:<path> <dest>:<path> For example: rclone copy ./genome-1.bam Georgie-OneDrive:Documents/Genome-1 Where I am transfering the file genome-1.bam to my pre-configured OneDrive remote named Georgie-OneDrive at the path Documents/Genome-1 To transfer data from rds or Artemis to OneDrive via a PBS job using the dtq queue: #!/bin/bash #PBS -P SIHsandbox #PBS -N transfer #PBS -q dtq #PBS -l select=1:ncpus=1:mem=20gb #PBS -l walltime=05:00:00 #PBS -W umask=022 #PBS -j oe #PBS -m e #PBS -M georgina.samaha@sydney.edu.au module load rclone source = # file/directory to transfer destination = # name of configured onedrive drive path = # onedrive path rclone copy ${ source } ${ destination } : ${ path }","tags":"unix","url":"https://sydney-informatics-hub.github.io/tidbits/cli-transfer-to-onedrive.html","loc":"https://sydney-informatics-hub.github.io/tidbits/cli-transfer-to-onedrive.html"},{"title":"Structure your data quickly and easily with Python dataclasses","text":"It can be a bit clunky to create a class in Python to store data, as you need to define an __init__ method and write some repetitive code to assign all the attributes: class Animal def __init__ ( name : str , age : int ): self . name = name self . age = age Python's new dataclass (in the standard library from Python 3.7 onward) makes this a lot quicker and more efficient, as it creates the __init__ method for you automatically, all you do is specify the types for the data: from dataclasses import dataclass @dataclass class Animal : name : str age : int dog = Animal ( name = \"Fido\" , age = 5 ) One of the big benefits of this is that your IDE/editor should automatically pick up on the types of your data and warn you when you're using it incorrectly. Using dataclasses can be a good alternative to returning multiple results in a dictionary or tuple, as it's easier to keep track of the different results. If you want something a bit more advanced than dataclasses , you can look at Pydantic , which allows you to define data types in a similar way, but also helps you convert data to and from other formats like JSON.","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/structure-your-data-quickly-and-easily-with-python-dataclasses.html","loc":"https://sydney-informatics-hub.github.io/tidbits/structure-your-data-quickly-and-easily-with-python-dataclasses.html"},{"title":"How to use a 'trace' to debug a crashing program in Linux","text":"When software crashes, it usually produces a runtime error and wil give you a hint to why it is failing. Digging into the libraries and system calls and monitoring \"where the program gets to\" can be informative. Expected execution user @host $ matlab MATLAB is selecting SOFTWARE OPENGL rendering . < M A T L A B ( R ) > Copyright 1984 - 2021 The MathWorks , Inc . R2021a Update 8 ( 9.10.0.2198249 ) 64 - bit ( glnxa64 ) February 15 , 2023 For online documentation , see https : // www . mathworks . com / support For product information , visit www . mathworks . com . >> disp ( \"hello world\" ) hello world >> Loudly crashing - enough info to search for a solution! user @host $ matlab java . lang . NullPointerException at javax . swing . JList $ AccessibleJList $ AccessibleJListChild . getAccessibleValue ( JList . java : 3400 ) at sun . lwawt . macosx . CAccessibility $ 31. call ( CAccessibility . java : 542 ) at sun . lwawt . macosx . CAccessibility $ 31. call ( CAccessibility . java : 534 ) at sun . lwawt . macosx . LWCToolkit $ CallableWrapper . run ( LWCToolkit . java : 511 ) at java . awt . event . InvocationEvent . dispatch ( InvocationEvent . java : 302 ) at java . awt . EventQueue . dispatchEventImpl ( EventQueue . java : 733 ) at java . awt . EventQueue . access $ 200 ( EventQueue . java : 103 ) at java . awt . EventQueue $ 3. run ( EventQueue . java : 694 ) at java . awt . EventQueue $ 3. run ( EventQueue . java : 692 ) ... Quietly crashing! How to debug!? user @host $ matlab MATLAB is selecting SOFTWARE OPENGL rendering . user @host $ ldd List Dynamic Dependencies (ldd), tells you all the libraries that your application requires. Often a missing package is the source of your problems. user @host $ ldd matlab linux - vdso . so .1 ( 0x00007ffe4d5d1000 ) libm . so .6 => / lib / x86_64 - linux - gnu / libm . so .6 ( 0x00007f3721a87000 ) libpthread . so .0 => / lib / x86_64 - linux - gnu / libpthread . so .0 ( 0x00007f3721866000 ) librt . so .1 => / lib / x86_64 - linux - gnu / librt . so .1 ( 0x00007f372165e000 ) libdl . so .2 => / lib / x86_64 - linux - gnu / libdl . so .2 ( 0x00007f372145a000 ) libstdc ++ . so .6 => / lib / x86_64 - linux - gnu / libstdc ++ . so .6 ( 0x00007f37210d6000 ) libmwmclmcrrt . so .11.1 => not found libgcc_s . so .1 => / lib / x86_64 - linux - gnu / libgcc_s . so .1 ( 0x00007f3720eba000 ) libc . so .6 => / lib / x86_64 - linux - gnu / libc . so .6 ( 0x00007f3720ad9000 ) / lib64 / ld - linux - x86 - 64. so .2 ( 0x00007f3721f10000 ) strace The output can be formidable, but the errors may be informative. Strace intercepts and records the system calls which are called by a process and the signals which are received by a process. user @ host $ strace - v matlab execve ( \"/opt/matlab/bin/glnxa64/MATLAB\" , [ \"/opt/matlab/bin/glnxa64/MATLAB\" ], [ \"MODULE_VERSION_STACK=3.2.10\" , \"HOSTNAME=hpc055\" , \"PBS_ACCOUNT=RDS-CORE-SIHnextgen-\" ... , \"TERM=xterm-256color\" , \"SHELL=/bin/bash\" , \"HISTSIZE=1000\" , \"TMPDIR=/tmp/pbs.6846183.pbsserve\" ... , \"PYTHON_PATH=:/home/nbut3013/ptBa\" ... , \"PBS_JOBNAME=STDIN\" , \"PBS_ENVIRONMENT=PBS_INTERACTIVE\" , \"QTDIR=/usr/lib64/qt-3.3\" , \"QTINC=/usr/lib64/qt-3.3/include\" , \"PBS_O_WORKDIR=/home/nbut3013\" , \"SINGULARITY_COMMAND=run\" , \"NCPUS=2\" , \"USER_PATH=/usr/local/singularity\" ... , \"USER=nbut3013\" , \"PBS_TASKNUM=1\" , \"LS_COLORS=rs=0:di=38;5;27:ln=38;\" ... , \"LD_LIBRARY_PATH=/.singularity.d/\" ... , \"PBS_O_HOME=/home/nbut3013\" , \"SINGULARITY_NAME=matlab-ubu.img\" , \"PBS_MOMPORT=15003\" , \"PBS_O_QUEUE=defaultQ\" , \"PBS_O_LOGNAME=nbut3013\" , \"PATH=/home/nbut3013/anaconda2/bi\" ... , \"MODULE_VERSION=3.2.10\" , \"MAIL=/var/spool/mail/nbut3013\" , \"PBS_O_LANG=en_AU.UTF-8\" , \"PBS_JOBCOOKIE=0000000038FCE62400\" ... , \"PWD=/project/RDS-CORE-SIHnextgen\" ... , \"_LMFILES_=/usr/local/Modules/mod\" ... , \"PBS_NODENUM=0\" , \"LANG=en_US.UTF-8\" , \"MODULEPATH=/usr/local/Modules/ve\" ... , \"TZ=Etc/UTC\" , \"PBS_JOBDIR=/home/nbut3013\" , \"LOADEDMODULES=singularity/3.7.0\" , \"SINGULARITY_ENVIRONMENT=/.singul\" ... , \"PS1=Singularity> \" , \"PBS_O_SHELL=/bin/bash\" , \"SINGULARITY_BIND=\" , \"PBS_JOBID=6846183.pbsserver\" , \"HISTCONTROL=ignoredups\" , \"SHLVL=2\" , \"HOME=/home/nbut3013\" , \"PBS_O_HOST=login3.hpc.sydney.edu\" ... , \"LOGNAME=nbut3013\" , \"QTLIB=/usr/lib64/qt-3.3/lib\" , \"CVS_RSH=ssh\" , \"PBS_QUEUE=interactive\" , \"MODULESHOME=/usr/local/Modules/3\" ... , \"PBS_O_MAIL=/var/spool/mail/nbut3\" ... , \"OMP_NUM_THREADS=2\" , \"LESSOPEN=||/usr/bin/lesspipe.sh \" ... , \"SINGULARITY_CONTAINER=/project/S\" ... , \"BASH_FUNC_module()=() { eval `/\" ... , \"PBS_O_SYSTEM=Linux\" , \"PBS_NODEFILE=/var/spool/PBS/aux/\" ... , \"G_BROKEN_FILENAMES=1\" , \"PBS_O_PATH=/home/nbut3013/anacon\" ... , \"_=/usr/bin/strace\" ]) = 0 brk ( NULL ) = 0x5d3000 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = - 1 ENOENT ( No such file or directory ) readlink ( \"/proc/self/exe\" , \"/opt/matlab/bin/glnxa64/MATLAB\" , 4096 ) = 30 access ( \"/etc/ld.so.preload\" , R_OK ) = - 1 ENOENT ( No such file or directory ) open ( \"/opt/matlab/bin/glnxa64/tls/x86_64/libmwi18n.so\" , O_RDONLY | O_CLOEXEC ) = - 1 ENOENT ( No such file or directory ) stat ( \"/opt/matlab/bin/glnxa64/tls/x86_64\" , 0x7ffff310fcb0 ) = - 1 ENOENT ( No such file or directory ) open ( \"/opt/matlab/bin/glnxa64/tls/libmwi18n.so\" , O_RDONLY | O_CLOEXEC ) = - 1 ENOENT ( No such file or directory ) stat ( \"/opt/matlab/bin/glnxa64/tls\" , 0x7ffff310fcb0 ) = - 1 ENOENT ( No such file or directory ) open ( \"/opt/matlab/bin/glnxa64/x86_64/libmwi18n.so\" , O_RDONLY | O_CLOEXEC ) = - 1 ENOENT ( No such file or directory ) stat ( \"/opt/matlab/bin/glnxa64/x86_64\" , 0x7ffff310fcb0 ) = - 1 ENOENT ( No such file or directory ) ... 3000 lines truncated ltrace Intercepts and records the dynamic library calls which are called by the executed process and the signals which are received by that process. It can also intercept and print the system calls executed by the program. user @host $ ltrace matlab memcpy ( 0x7ffcc608fc70 , \"int8\" , 4 ) = 0x7ffcc608fc70 memcpy ( 0x990c50 , \"int8\" , 4 ) = 0x990c50 memcpy ( 0x7ffcc608fc70 , \"int16\" , 5 ) = 0x7ffcc608fc70 memcpy ( 0x7ffcc608fc70 , \"int16\" , 5 ) = 0x7ffcc608fc70 memcpy ( 0x990ca0 , \"int16\" , 5 ) = 0x990ca0 memcpy ( 0x7ffcc608fc70 , \"int32\" , 5 ) = 0x7ffcc608fc70 memcpy ( 0x7ffcc608fc70 , \"int32\" , 5 ) = 0x7ffcc608fc70 memcpy ( 0x990cf0 , \"int32\" , 5 ) = 0x990cf0 memcpy ( 0x7ffcc608fc70 , \"int64\" , 5 ) = 0x7ffcc608fc70 memcpy ( 0x7ffcc608fc70 , \"int64\" , 5 ) = 0x7ffcc608fc70 memcpy ( 0x990d40 , \"int64\" , 5 ) = 0x990d40 memcpy ( 0x7ffcc608fc70 , \"uint8\" , 5 ) = 0x7ffcc608fc70","tags":"unix","url":"https://sydney-informatics-hub.github.io/tidbits/how-to-use-a-trace-to-debug-a-crashing-program-in-linux.html","loc":"https://sydney-informatics-hub.github.io/tidbits/how-to-use-a-trace-to-debug-a-crashing-program-in-linux.html"},{"title":"Find github pages","text":"To gauge how many github repos have a \"github pages\" within a particular an organisation, you can run: gh api -H \"Accept: application/vnd.github+json\" /orgs/Sydney-Informatics-Hub/repos --jq '.[] | {name: .name, has_pages}' --paginate | grep true This returns output like: { \"has_pages\" : true , \"name\" : \"training.artemis.introhpc\" } { \"has_pages\" : true , \"name\" : \"training.artemis\" } { \"has_pages\" : true , \"name\" : \"training.home\" } { \"has_pages\" : true , \"name\" : \"workshopbmc\" } { \"has_pages\" : true , \"name\" : \"lessonbmc\" } { \"has_pages\" : true , \"name\" : \"2018_12_10_GISworkshop\" } { \"has_pages\" : true , \"name\" : \"training.artemis.interhpc\" } { \"has_pages\" : true , \"name\" : \"training-template-old\" } { \"has_pages\" : true , \"name\" : \"training.artemis.gpu\" } { \"has_pages\" : true , \"name\" : \"190318_MLR\" } { \"has_pages\" : true , \"name\" : \"training-RNAseq\" } { \"has_pages\" : true , \"name\" : \"190408_MLpy\" } { \"has_pages\" : true , \"name\" : \"training-RNAseq-slides\" } { \"has_pages\" : true , \"name\" : \"training.artemis.rds\" } { \"has_pages\" : true , \"name\" : \"codeofconduct\" } { \"has_pages\" : true , \"name\" : \"training.artemis.python\" } { \"has_pages\" : true , \"name\" : \"lessons-mlpy\" } { \"has_pages\" : true , \"name\" : \"elfr\" } { \"has_pages\" : true , \"name\" : \"recocam\" } { \"has_pages\" : true , \"name\" : \"training.artemis.r\" } { \"has_pages\" : true , \"name\" : \"geopython\" } { \"has_pages\" : true , \"name\" : \"simplelesson\" } { \"has_pages\" : true , \"name\" : \"training.gadi.intro\" } { \"has_pages\" : true , \"name\" : \"jenkins_docker_test\" } { \"has_pages\" : true , \"name\" : \"training.argus.gpu\" } { \"has_pages\" : true , \"name\" : \"training.blockchain\" } { \"has_pages\" : true , \"name\" : \"geopython-bhp\" } { \"has_pages\" : true , \"name\" : \"stats-resources\" } { \"has_pages\" : true , \"name\" : \"geopython-pawsey\" } { \"has_pages\" : true , \"name\" : \"BioCommons_RNASeq_dryRun\" } { \"has_pages\" : true , \"name\" : \"training-template\" } { \"has_pages\" : true , \"name\" : \"intro-git\" } { \"has_pages\" : true , \"name\" : \"AgReFed-Workshop\" } { \"has_pages\" : true , \"name\" : \"rna-seq-pt2-quarto\" } { \"has_pages\" : true , \"name\" : \"rna-seq-pt1-quarto\" } { \"has_pages\" : true , \"name\" : \"training.RNAseq.series-quarto\" } { \"has_pages\" : true , \"name\" : \"dataharvester\" } { \"has_pages\" : true , \"name\" : \"ParallelPython\" } { \"has_pages\" : true , \"name\" : \"geodata-harvester\" } { \"has_pages\" : true , \"name\" : \"PIPE-3034-obesity2\" } { \"has_pages\" : true , \"name\" : \"customising-nfcore-workshop\" } { \"has_pages\" : true , \"name\" : \"masterclass_RStudio_GitHub\" } { \"has_pages\" : true , \"name\" : \"RStudio_github_versioncontrol\" } { \"has_pages\" : true , \"name\" : \"lessons_mlr_tidymodels\" } { \"has_pages\" : true , \"name\" : \"Git_RStudio_masterclass\" }","tags":"git","url":"https://sydney-informatics-hub.github.io/tidbits/find-github-pages.html","loc":"https://sydney-informatics-hub.github.io/tidbits/find-github-pages.html"},{"title":"Automating linting and error checking with Trunk","text":"See Trunk in action in the eeharvest package repository Use Trunk to check and monitor code prior to pushing it to production, allowing you to catch issues quickly. It works like a local CI or pre-commit hook for linting and formatting, but is instantenous . Best of all, it doesn't force collaborators to install anything, and they may appreciate you for it. With trunk you can replace: linters: e.g. flake8 , pylint , eslint , stylelint , shellcheck , markdownlint formatters: e.g. black , prettier , isort , shfmt issue detection: e.g. bandit , safety , mypy ErrorLens : in-line error reporting - installing this will result in \"double\" error reporting All of the above are configured, installed and managed for you by Trunk, and editable in a .trunk.yaml file. Lint Trunk can lint your files as you type and shows you the errors inline: Format Enable automatic formatting on save - using black , prettier , or other formatters of choice: Trunk is available for free in most circumstances. Check When used in VS Code, Trunk can consolidate all issues in a \"Check\" sidebar: Other functionality Trunk works with continuous integration workflows, accepts custom linters and parsers, and has a robust CLI interface which allows developlers to allow teammates to use its features without installing anything (if Trunk is commited directly into the repo). Check out the documentation here . In most cases, a local install of Trunk is sufficient for routine linting and error checks. It's just so convenient. Installation VS Code (recommended) Install via VS Code extensions . For every new project, you will be asked if you want to initialise Trunk. Bash If you use other editors, you can install Trunk via the command line: curl https://get.trunk.io -fsSL | bash Then, initialise trunk for a project by running the following command in the root folder: trunk init Use trunk check to run all linters, or trunk fmt to run all formatters. More information ca be found in the documentation here .","tags":"misc","url":"https://sydney-informatics-hub.github.io/tidbits/automating-linting-and-error-checking-with-trunk.html","loc":"https://sydney-informatics-hub.github.io/tidbits/automating-linting-and-error-checking-with-trunk.html"},{"title":"Standardised Python testing with Tox and `tox-conda`","text":"Tox is a great too that standardises package building, testing, linting and CI-integration for Python projects. As not everyone builds Python packages, this article will focus on automatic unit tests for any python code. Installing tox is as simple as: pip install tox Or if you use conda/mamba: conda install -c conda-forge tox Tox and eeharvest The eeharvest package uses Tox to automate a bunch of things: building the package running tests using pytest in multiple environments checking code coverage publishing to PyPI If configured properly, the first four actions can be done in a single command: tox Publishing the package to PyPI is as simple as: tox -e publish -- --repository pypi Below is a preview of what happens when using tox for unit testing. If you look closely, tox is running unit tests on Python versions 3.8 to 3.11 (6 environments in total), and then publishing the coverage for each environment: Configuration Tox works by looking at a tox.ini file in the root of the project. That's it. A single file. A simple configuartion for two test environments, Python 3.8 and 3.11, can be generated with the following configuration settings in the file: [tox] requires = tox> = 4 env_list = py{38,311} requires = tox-conda [testenv] description = Invoke pytest to run automated tests setenv = TOXINIDIR = {toxinidir} extras = testing deps = pytest> = 7 pytest-sugar conda_deps = geedim conda_channels = conda-forge conda_install_args = --override-channels commands = pytest {posargs : tests} Note that the above config requires use of tox-conda , which can be installed via mamba/conda: mamba install - c conda - forge tox - conda Tox will automatically configure conda environments, because requires = tox-conda is set. For each environment specified in env_list , it will install dependencies specified in conda_deps and deps (pip dependencies), and then run tests in your tests folder. For more information on configuring tox, see the official documentation .","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/standardised-python-testing-with-tox-and-tox-conda.html","loc":"https://sydney-informatics-hub.github.io/tidbits/standardised-python-testing-with-tox-and-tox-conda.html"},{"title":"Use git on VMs easily and securely with SSH agent forwarding","text":"If you use git repos on a VM (or other remote server), you'll know the pain of entering your login details whenever you need to pull down new code. To avoid this, you can use SSH agent forwarding to use your local SSH keys in a remote session. If you're already using SSH to log in to the server, you just need to add the -A option to your SSH command, like: ssh -A username@host With SSH forwarding: You don't have to enter your username/password repeatedly on the remote server. You don't have to copy SSH keys to the server, which might create security issues. On the server, you need to use the SSH version of the repo's URL, e.g. git@github.com:USERNAME/REPOSITORY.git . If you're currently using the HTTPS version you can switch with: git remote set-url git@github.com:USERNAME/REPOSITORY.git If you're just starting out with SSH and git and don't understand the above, GitHub has good documentation on how to get set up with it.","tags":"misc","url":"https://sydney-informatics-hub.github.io/tidbits/use-git-on-vms-easily-and-securely-with-ssh-agent-forwarding.html","loc":"https://sydney-informatics-hub.github.io/tidbits/use-git-on-vms-easily-and-securely-with-ssh-agent-forwarding.html"},{"title":"Siuba - Scrappy data analysis in Python you would normally do in R","text":"Suiba https://siuba.readthedocs.io/en/latest/ Siuba is a python package that replicates R dplyr language in python. Provides concise, flexible data-analysis over multiple data sources currently including pandas DataFrames and SQL tables. Siuba is build on top of pandas, hence data methods refrence pandas operations you will know already if you use pandas often. select() - keep certain columns of data. filter() - keep certain rows of data. mutate() - create or modify an existing column of data. summarize() - reduce one or more columns down to a single number. arrange() - reorder the rows of data. Basic example below. Notice: lazy expression _ avoids duplicating name of dataframe in operations the _. when referencing columns '>>' is the new pipe as opposed to %>% in R # Load packages from siuba import _ , group_by , summarize , mutate , arrange from siuba.data import mtcars ( mtcars >> group_by ( _ . cyl ) >> summarize ( avg_hp = _ . hp . mean ()) ) ( mtcars >> group_by ( _ . cyl ) >> mutate ( var = _ . hp - _ . hp . mean ()) >> arrange ( _ . cyl )) Lazy expression provides concise syntax which also reduces the need for many lambda expressions. # old approach: repeat name mtcars [ mtcars . cyl == 4 ] # old approach: lambda mtcars [ lambda _ : _ . cyl == 4 ] # siu approach mtcars [ _ . cyl == 4 ] #Less lambdas # pandas mtcars . assign ( avg = lambda d : d . mpg . mean ()) # siuba mutate ( mtcars , avg = _ . mpg . mean ()) Suiba works with plotnine (the ggplot equivalent for python) to make python feel alot like R in data manipulation and plotting. from siuba import mutate , _ from plotnine import ggplot , aes , geom_point ( mtcars >> mutate ( hp_per_cyl = _ . hp / _ . cyl ) >> ggplot ( aes ( \"cyl\" , \"hp_per_cyl\" )) + geom_point () ) Nest and Unnest operations available. from siuba import _ , mutate , unnest import pandas as pd tagged = pd . DataFrame ({ 'id' : [ 1 , 2 , 3 ], 'tags' : [ 'a,b,c' , 'd,e' , 'f' ] }) ( tagged >> mutate ( split_tags = _ . tags . str . split ( ',' )) >> unnest ( \"split_tags\" ) ) Using databases: Interaction is the same if data is remote data such as sql table, rather than datframe. Generating the SQL query that corresponds to the data manipulation is also available with show_query(). from sqlalchemy import create_engine from siuba.sql import LazyTbl from siuba import show_query # copy in to sqlite engine = create_engine ( \"sqlite:///:memory:\" ) mtcars . to_sql ( \"mtcars\" , engine , if_exists = \"replace\" ) # connect with siuba tbl_mtcars = LazyTbl ( engine , \"mtcars\" ) query_data = ( tbl_mtcars >> group_by ( _ . cyl ) >> summarize ( avg_hp = _ . hp . mean ()) ) #Under the hood siuba's summarize function is converting the lazy expression show in the code below to SQL query_data >> show_query () Comparison with other similar packages are available here: https://siuba.readthedocs.io/en/latest/key_features.html#Key-features Other notable points include fast group by operations and abstract syntax trees for transforming operations.","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/siuba-scrappy-data-analysis-in-python-you-would-normally-do-in-r.html","loc":"https://sydney-informatics-hub.github.io/tidbits/siuba-scrappy-data-analysis-in-python-you-would-normally-do-in-r.html"},{"title":"Shell Tips","text":"A collection of misc commands and short snippets that might make your life a lot easier when using the shell locally, on remote, or on hpc. Table of Contents Check permissions Change permissions Count all files Count all files of a certain type Force Quit a Pesky App That Won't Quit HPC job resource monitoring Find all .txt files and then run some code on each of them Remove any line not beginning with \"apples\" from text files in this folder Rsync from local folder to folder on HPC Set up and use ssh keys for remote access to RDS Check permissions ls -lah Change permissions If you want to change the permissons for a directory and its contents, use this: chmod a+rwx <some_dir> -R Where a is all users, rwx means Read + Write + eXecute, <some_dir> is a directory you want to change permissions for, and -R applies these changes to all contents recursively. Can also use this chmod calculator to generate specific permissions in numerical and symbolic formats. Count all files find . -type f | wc -l Count all files of a certain type(s) There are several ways to count all files of a certain type(s), but this is by far the fastest simple shell command. For example, if you want to recursively look through a directory and count all files of several different extentsions (in this case images): find . -type f | sed -e 's/.*\\.//' | sort | uniq -c | sort -n | grep -Ei '(tiff|bmp|jpeg|jpg|png|gif)$' Find all .txt files and then run some code on each of them find -name \"*.txt\" -exec <command> Remove any line not beginning with \"apples\" from text files in this folder sed -i '' '/&#94;apples/!d' *.txt Force quit a pesky app that won't quit Imagine you are trying to quit an application (e.g. Docker) or killa process using activity monitor, but for some reason the application keeps coming back or won't quit. osascript -e 'quit app \"Docker\"' This will effecitevly nuke the app for you. Rsync from local folder to folder on HPC rsync -tvxPr /local/path/to/files unikey@hpc.sydney.edu.au:/remote/path/to/copy/to Check when your HPC job should start If you have a bunch of jobs in the queue on artemis, you might want to check when they are expected to start. qstat -u <your_unikey> -T HPC job resource monitoring Scripts availabile here will pull compute (CPU, memory, walltime etc) resource consumption from PBS job logs into tab-delimited format and to report queue time from job history. Usage reporting scripts are currently available for: University of Sydney's Artemis National Compute Infrastructure's Raijin National Compute Infrastructure's Gadi University of Queensland's Flashlite Run the script from within the directory that contains the usage log files to be read. Optionally, can include the prefix of the usage logs as an argument on the command line. For example, to collate resource usage from Gadi PBS job logs, run: perl /g/data/er01/gadi_usage_report_v1.1.pl <log file prefix> To live fast and free, add the following alias to your .bashrc profile: alias joblogs='perl /g/data/er01/HPC_usage_reports/gadi_usage_report_v1.1.pl' And run from within the directory housing log files, with: joblogs An example of the output: #JobName CPUs_requested CPUs_used Mem_requested Mem_used CPUtime CPUtime_mins Walltime_req Walltime_used Walltime_mins JobFS_req JobFS_used Efficiency Service_units Job_exit_status Date Time amber_T115991B2.o 16 16 190.0GB 65.47GB 03:58:20 238.33 02:00:00 00:41:19 41.32 100.0MB 0B 0.36 65.42 0 2022-05-09 18:10:57 amber_T519706C1.o 16 16 190.0GB 73.16GB 03:35:27 215.45 02:00:00 00:37:43 37.72 100.0MB 0B 0.36 59.72 0 2022-05-09 18:06:46 amber_T530707.o 16 16 190.0GB 73.96GB 02:53:58 173.97 02:00:00 00:33:51 33.85 100.0MB 0B 0.32 53.60 0 2022-05-09 18:02:30 amber_T531207.o 16 16 190.0GB 70.38GB 03:25:54 205.90 02:00:00 00:37:04 37.07 100.0MB 0B 0.35 58.69 0 2022-05-09 18:05:28 amber_T547109.o 16 16 190.0GB 73.53GB 03:33:23 213.38 02:00:00 00:42:44 42.73 100.0MB 0B 0.31 67.66 0 2022-05-09 18:10:41 amber_T563810.o 16 16 190.0GB 68.73GB 02:55:47 175.78 02:00:00 00:30:21 30.35 100.0MB 0B 0.36 48.05 0 2022-05-09 17:57:43 How to SCP if there are spaces in folder names Add three \\\\\\ after every space. Like so: folder \\\\\\ with \\\\\\ spaces/* Set up and use ssh keys for remote access to RDS Connecting to RDS (research-data-ext.sydney.edu.au) remotely is only permissable via an SFTP connection. If you're working on a server external to the University (i.e. NCI Gadi) and need to transfer data to and from the RDS and this system, you can connect to the RDS on the command line with: sftp < UNIKEY > @research - data - ext . sydney . edu . au You will be prompted to provide your password with: Authorised users only. All activity may be monitored and reported. Password: However, if you are attempting to connect to RDS and transfer files via a PBS job, where the scheduling of these operations is driven by a batch controlled workflow, relying on entering a password or passphrase at the time of the operation is not feasible. In this case, you will need to set up password-free ssh keys . You can do this with the following process: On the remote server, wanting to access RDS 1. Log into the remote server 2. cd ~ 3. ls .ssh (If this doesnt exist, then mkdir .ssh ) 4. chmod 700 .ssh 5. ssh-keygen Just press enter when prompted, saving the key in ~/.ssh/id_rsa and enter for no passphrase 6. cd .ssh You will now see two files, id_rsa (private key) and id_rsa.pub (your public key) 7. chmod 700 id_rsa* 8. cat ~/.ssh/id_rsa.pub >> authorized_keys this will create the authorized_keys file that you will copy to RDS to allow password-free connection 9. chmod 700 authorized_keys 10. sftp <UNIKEY>@research-data-ext.sydney.edu.au 11. You will be prompted to provide your password, same as above. Once your login is successful you will be in remotely logged into your home directory on RDS/Artemis. 12. cd .ssh If this doesn't exist, then run mkdir ~/.ssh 13. put authorized_keys This will transfer authorized_keys on Gadi to your current directory. With sftp, it will look for the file relative to where you launched sftp. You can check where you are on Gadi using lls 14. Logout using ctrl+z and 15. Test the sftp connection again by trying to log in, same as above. You should not need to use a password this time. Something to keep in mind Using ssh keys is very sensitive to file and directory permissions on both sides â€“ and it looks not just at the key itself, but also the .ssh folder and your home directory. If you somehow managed to mess your keys up (like me) and cannot get the password-free method to work, you probably have messed up permissions of your ~/.ssh directories, as well as the authorized_keys and id_rsa.pub files.","tags":"misc","url":"https://sydney-informatics-hub.github.io/tidbits/shell-tips.html","loc":"https://sydney-informatics-hub.github.io/tidbits/shell-tips.html"},{"title":"Use logging instead of print","text":"Like a lot of people, I'm guilty of using print() to debug my code. While it's a good idea to use a proper debugger, it can also be good to have your programs log useful information as they run, particularly if they're going to run on a server relatively unsupervised. That's where Python's logging module comes in. It's best to use logging instead of print right from the start of a project, so keep it in mind for your next one! At the most basic level, logging lets you set the importance level of each message you're sending. The levels (in order of importance) are: DEBUG INFO WARNING ERROR CRITICAL To log each kind of message, just use the equivalent function from logging : logging . debug ( \"Log a debug message\" ) logging . warning ( \"Log a warning\" ) # etc. When running your code, you can set the log level (using a command line argument, environment variable, config file, etc.), and only messages of that importance or higher will be shown: logging . basicConfig ( level = logging . INFO ) logging . debug ( \"This won't be shown\" ) logging . info ( \"Anything at info level or higher will be logged\" ) The benefit of this is you can have your programs show a lot of information on what they're doing through debug messages, without making the output too noisy for regular use. If you find yourself going back through your code removing print() statements once it's finished, consider using logging instead. Going further, the logging module has lots of useful functionality like: Logging to a file instead of/as well as the console - and easily switching between these Automatically including context like the current Python module and line number in messages Running multiple loggers with different settings. See this RealPython tutorial for info on some of these. The closest equivalent in R seems to be the logger package, which is inspired by Python's logging.","tags":"python","url":"https://sydney-informatics-hub.github.io/tidbits/use-logging-instead-of-print.html","loc":"https://sydney-informatics-hub.github.io/tidbits/use-logging-instead-of-print.html"},{"title":"Improve your email communications skills!","text":"Use active voice! Academics use a lot of passive voice. It's a hard habit to break! Active voice is usually simpler, clearer, and more interesting to read. Here's an anonymous example from someone at SIH: As part of this process we will be starting to offer options for in-person workshops Why not just say We're offering in-person workshops Use calls to action Think about what you want the reader to do after reading your story. Stories about interesting projects are great, but do you just want them to sit and think about the mysteries of the universe? Every marketing email you get from a charity, business, political party will include a clear call to action , a prompt to actually do something with the information. If you just want people to read the article , that's great! Other good prompts can include: Apply online Contact us Tell us your thoughts Be a bit less formal Stick to conversational, informal language in emails. That means using real words like \"Professor\" instead of abbreviations like \"Assoc. Prof.\", and \"Doctor Y and her team\" rather than \"Y et al.\"","tags":"misc","url":"https://sydney-informatics-hub.github.io/tidbits/improve-your-email-communications-skills.html","loc":"https://sydney-informatics-hub.github.io/tidbits/improve-your-email-communications-skills.html"},{"title":"Modernize your Python code with type annotations","text":"Python has supported optional type annotations since Python 3.6, like: # Specify argument and return types def add ( x : int , y : int ) -> int : return x + y All currently supported Python versions support them now, so you can start adding them to your code without breaking anything. Type annotations are entirely optional and are basically ignored by the Python compiler, so you won't break your code by including them. However, once you start adding them to your code you should find that your editor/IDE can give you better autocompletion for object methods (because it can determine their type), and can catch errors in how you're using your functions: For more detailed and rigorous checking of your code-base, you can use tools like mypy , which scans your code-base to ensure all types are consistent. Annotation tools can be found in the typing module in the standard library. Some of the most common useful type annotations include using the Optional annotation to document when an argument can be None : from typing import Optional def append_data ( value : float , current_data : Optional [ list ] = None ): if current_data is None : current_data = [] current_data . append ( value ) Or specifying the types for your nested data structures: # In newer versions of Python (>= 3.9), you can just use # list or tuple directly from collections import defaultdict from typing import Dict , List , Tuple def get_total_scores ( scores : List [ Tuple [ str , int ]]) -> Dict [ str , int ]: totals = defaultdict ( int ) for person , score in scores : totals [ person ] += score return totals get_total_scores ([ ( 'Alice' , 3 ), ( 'Bob' , 2 ), ( 'Alice' , 1 ) ])","tags":"python","url":"https://sydney-informatics-hub.github.io/tidbits/modernize-your-python-code-with-type-annotations.html","loc":"https://sydney-informatics-hub.github.io/tidbits/modernize-your-python-code-with-type-annotations.html"},{"title":"The world's shortest tidbit: use #%% in Python scripts","text":"The world's shortest tidbit I don't think this is really worthwhile as a tidbit, as it's (1) very short, and (2) very obvious - once you know about it. But this is a productivity lifehack that makes it much easier to use Python for data science, through both VSCode and PyCharm, kind of how easy R is to use via RStudio. The tidbit is: If you break up the code in your .py script with comments of the format #%% this will get both VSCode and PyCharm to recognise the code below them, until the next #%% , as a code cell (what R calls a \"code chunk\"). This has the benefits: You can execute code cell-by-cell (and not just line by line) using keyboard shortcuts. For example, in the image below, I used shift-Return to execute all of the imports at once. When live-debugging and working on your code you can get a jupyter-notebook-like interface (aka console, for the R users) on the right, where you can type extra commands that you want to use to develop and prototype your code as you go along. Because your actual code is stored in a .py file, it is nicely amenable to version control with git - making it like Rmd files in this regard. Conversely, jupyter notebook output is also stored with the notebook, so can look like a binary mess with git - although GitHub does try to ameliorate this problem somewhat. So there really are heaps of benefits - and no drawbacks I've been able to see - for using # %% in your Python code.","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/the-worlds-shortest-tidbit-use-in-python-scripts.html","loc":"https://sydney-informatics-hub.github.io/tidbits/the-worlds-shortest-tidbit-use-in-python-scripts.html"},{"title":"Design patterns (Part 1)","text":"Design Patterns (Part 1): What? Why? What is a Design Pattern In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design. source: Wikipedia Why use Design Patterns Tested and adopted by the wider software community Re-usable pieces of code Common language to solve problems in software design and development ... Sources: * Wikipedia - Software Design Pattern * Refactoring - Design Patterns Design Patterns in Python : an example that I used in my projects The Singleton Design Pattern Ensure a class has only one instance, and provide a global point of access to it. source: Wikipedia Code class MySingletonClass ( object ): __instance = None def __new__ ( cls , * args , ** kwargs ): if cls . __instance is None : cls . __instance = super ( MySingletonClass , self ) . __new__ ( cls ) return cls . __instance def __init__ ( self , some_argument ): self . some_argument = some_argument or another variant: class MySingletonClass ( object ): def __new__ ( cls , * args , ** kwargs ): if not hasattr ( cls , __instance ): cls . __instance = super ( MySingletonClass , self ) . __new__ ( cls ) return cls . __instance . . . Singleton to inherit from (multiple classes can inherit without getting confused): class Singleton ( type ): _instances = {} def __call__ ( cls , * args , ** kwargs ): if cls not in cls . _instances : cls . _instances [ cls ] = ( super ( Singleton , cls ) . __call__ ( * args , ** kwargs ) ) return cls . _instances [ cls ] class MySingletonClass ( Singleton ): \"\"\"docstring for MySingletonClass\"\"\" def __init__ ( self , arg ): self . arg = arg there are many more definitions if you Google it . Usage: How do I know that is really a Singleton? Here below I demonstrate how to check if you did it right. class MyClassSingleton ( object ): __instance = None def __new__ ( cls , * args , ** kwargs ): if cls . __instance is None : cls . __instance = super ( MyClassSingleton , cls ) . __new__ ( cls ) return cls . __instance def __init__ ( self , arg1 , arg2 ): self . arg1 = arg1 self . arg2 = arg2 MyClassSingleton ( 1 , 2 ) <__main__.MyClassSingleton at 0x7feac43f2c40> class1 = MyClassSingleton ( \"this\" , \"that\" ) class2 = MyClassSingleton ( \"this\" , \"that\" ) class1 is class2 True hex ( id ( class1 )), hex ( id ( class2 )) ('0x7feac43f2c40', '0x7feac43f2c40') class MyClassNotSingleton ( object ): def __init__ ( self , arg1 , arg2 ): self . arg1 = arg1 self . arg2 = arg2 class1 = MyClassNotSingleton ( \"this\" , \"that\" ) class2 = MyClassNotSingleton ( \"this\" , \"that\" ) class1 is class2 False hex ( id ( class1 )), hex ( id ( class2 )) ('0x7feac440cf40', '0x7feac440cb50') How did I use it? creating connection to database creating connection Dask cluster: no need to pass it through functions, but just create an instance of it to return always the same object But can be used also for: get some data from something only once and save it as attribute in a class ... (google for more examples) Coming soon ... Design Patterns Part 2: more design patterns that I used Design Patterns in R More Reading Medium - The 7 Most Important Software Design Patterns google \"Design Patterns in ...\"","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/design-patterns-part-1.html","loc":"https://sydney-informatics-hub.github.io/tidbits/design-patterns-part-1.html"},{"title":"CUDA VS CPU: BERT embedding benchmarks","text":"cuda-vs-cpu-example CUDA is faster than CPU for deep learning, but how fast are we talking about? Let's find out! Introduction At the core of most of the popular whizbang deep learning methods are complex matrix operations. These operations are very taxing for CPUs. However, the architecture of GPUs, particularly NVIDIA's RTX consumer gaming cards and various enterprise cards is far more ammenable to deep learning. Last year, Chao and I developed a pipeline for using Google's BERT (Bidirectional Encoder Representations from Transformers) to classify different portions of text in academic journal articles. We used Pytorch and Huggingface's Transformers to implement a pre-trained BERT model that had been trained on scientific texts (SciBERT). While this pipeline gave us very impressive results, it was extremely CPU intensive. We ended up having to deploy it at scale on a virtual machine, to prevent melting our MacBooks(I honestly wonder if my aggressive use of BERT on my MacBook contributed to its freaky phantom kernal panics...suffice to say you DO NOT want those!). All along the way, I kept asking myself: how much faster would this be with CUDA? Without further adieou, let's find out!!! Scenario: Using BERT We built a Python pipeline to generate SciBERT sentence embeddings from full texts of academic journal articles; these embeddings would then be passed to a random forest classifier model to predict whether a particular sentence was conflict of interest, funding statement, or anything else. Here's the portion of the pipeline concerned that we are interested in: device = torch . device ( \"cpu\" ) log = logging . getLogger () logging . basicConfig ( level = logging . INFO ) warnings . filterwarnings ( \"ignore\" ) ap = argparse . ArgumentParser () ap . add_argument ( \"--text-dir\" , default = \"./texts\" , type = pathlib . Path ) ap . add_argument ( \"-o\" , \"--out-path\" , default = \"./pickles\" , type = pathlib . Path , ) args = ap . parse_args () \"\"\" A function to extract text from Academic pdfs that have been converted into DOCX files. This function excludes content from the references section onwards. To include references and onwards, remove the if statements from this function. \"\"\" # TODO : incorporate pdf -> DOCX conversion via Acrobat Pro API ? # TODO : strip images from text def getText ( filename ) : \"\"\" Extract text from a DOCX file, excluding tables. \"\"\" doc = docx . Document ( filename ) fullText = [] for para in doc . paragraphs : para . text = re . sub ( r \"-\\n(\\w+ *)\" , r \"\\1\\n\" , para . text ) para . text = re . sub ( r \"\\s\\s+\" , \" \" , para . text ) para . text = re . sub ( r \"-\\s(\\w+ *)\" , r \"\\1\\n\" , para . text ) if para . style . name . startswith ( \"Heading\" ) and para . text . startswith ( ( \"References\" , \"references\" , \"REFERENCES\" , \"Bibliography\" , \"Citations\" ) ) : break if para . text . startswith ( ( \"References\" , \"references\" , \"REFERENCES\" , \"Bibliography\" , \"Citations\" ) ) : break fullText . append ( para . text . strip ( \"\\n\" )) return \" \" . join ( fullText ) def check_if_file_exists ( self , pom_file_path , extension ) : return pom_file_path . with_suffix ( extension ). exists () def sent_span ( temp ) : \"\"\" Perform sentence segmentation, and then generate spans describing these sentences. \"\"\" sentences = nltk . sent_tokenize ( temp ) spans = [] for sent in reversed ( sentences ) : st = temp . rfind ( sent ) spans . append (( st , st + len ( sent ))) temp = temp [ :st ] spans . reverse () return spans def sents ( temp ) : \"\"\" Perform sentence segmentation. \"\"\" sentences = nltk . sent_tokenize ( temp ) return sentences def flatten_text ( r ) : \"\"\" Squish the nested sentences. \"\"\" data = [] for i , s in enumerate ( r [ \"sent_spans\" ] ) : temp = { \"filename\" : r [ \"filename\" ] . replace ( \".docx\" , \"\" ), \"span\" : s , \"location\" : round ( 100 * (( s [ 0 ] + s [ 1 ] ) / 2 ) / r [ \"length\" ] , 2 ), \"sentence\" : r [ \"text\" ][ s[0 ] : s [ 1 ] ] , } data . append ( temp ) return pd . DataFrame . from_dict ( data ) # SciBERT tokenizer = AutoTokenizer . from_pretrained ( \"allenai/scibert_scivocab_uncased\" ) model = AutoModel . from_pretrained ( \"allenai/scibert_scivocab_uncased\" ) # TODO : check to see if input file has already been turned into a pickle \"\"\" Loop through files in the text directory: 1. read each one in, 2. sentence segmentation, 3. tokenize using SciBERT, 4. generate SciBERT embeddings, and 5. export as pickles. \"\"\" progress = tqdm ( args . text_dir . glob ( \"*.docx\" )) for text_path in progress : if os . path . isfile ( str ( args . out_path ) + \"/\" + text_path . name + '.pkl' ) : print ( text_path . name + \" has already been processed\" ) continue progress . set_description ( text_path . name ) corpus_list = [] index = [] filename = [] cur_corpus = getText ( text_path ) corpus_list . append ( cur_corpus ) index . append ( text_path ) filename . append ( os . path . basename ( text_path )) df = pd . DataFrame ( index = index , data = { \"filename\" : filename , \"text\" : corpus_list } ) df = df . assign ( doc_words = df . text . apply ( lambda t : len ( t . split ()))) # Word Counts df = df . assign ( doc_characters = df . text . str . replace ( \"[\\s\\n\\t]\" , \"\" ). str . len () ) # Character Counts df = df . assign ( sent_spans = df . text . apply ( lambda text : sent_span ( text ))) df = df . assign ( length = df . text . str . len ()) df = df . assign ( sents = df . text . apply ( lambda text : sents ( text ))) sent_df = pd . concat ( [ flatten_text(df.loc[idx ] ) for idx in df . index ] ) sent_df [ \"sentence\" ] = sent_df [ \"sentence\" ] . astype ( \"str\" ) sent_df [ \"sentence\" ] = sent_df [ \"sentence\" ] . apply ( lambda x : x [ :512 ] ) # Cut all character strings to a max length of 512 characters . sent_df = sent_df [ sent_df[\"sentence\" ] . str . contains ( \"[A-Za-z]\" , na = False ) ] # Remove any rows that contains only non - alphanumeric characters . sent_df = sent_df . dropna () # Remove any lines that are NA . sent_df = sent_df . reset_index ( drop = True ) idx = sent_df . index tokenized = sent_df [ \"sentence\" ] . apply ( ( lambda x : tokenizer . encode ( x , add_special_tokens = True )) ) max_len = 0 for i in tokenized . values : if len ( i ) > max_len : max_len = len ( i ) padded = np . array ( [ i + [0 ] * ( max_len - len ( i )) for i in tokenized . values ] ) attention_mask = np . where ( padded != 0 , 1 , 0 ) # input_ids = torch . tensor ( padded ) input_ids = torch . tensor ( padded ). to ( torch . int64 ) attention_mask = torch . tensor ( attention_mask ) with torch . no_grad () : last_hidden_states = model ( input_ids , attention_mask = attention_mask ) features = last_hidden_states [ 0 ][ :, 0, : ] . numpy () features = sent_df . join ( pd . DataFrame ( features , index = idx ), rsuffix = \"_\" ) output_str = str ( args . out_path ) base = os . path . basename ( text_path . name ) text_str = str ( base ) output = base . replace ( \".docx\" , \".pickle\" ) output = output_str + \"/\" + output with open ( output , \"wb\" ) as output_file : pickle . dump ( features , output_file ) Ok, that's a lot of stuff to look at. Lets just look at the parts that are involved with running PyTorch on either CPU or GPU: device = torch.device(\"cpu\") # SciBERT tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\") model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\") # input_ids = torch.tensor(padded) input_ids = torch.tensor(padded).to(torch.int64) attention_mask = torch.tensor(attention_mask) with torch.no_grad(): last_hidden_states = model(input_ids, attention_mask=attention_mask) features = last_hidden_states[0][:, 0, :].numpy() With a few simple changes, we can tell our install of PyTorch to use CUDA. NOTE: on MacOS your PyTorch install will by default be the CPU only version. To work with CUDA and GPUs, you should use a computer with a compatible GPU and OS (Linux or Windows are the best for now). # Check if you have CUDA available if torch.cuda.is_available(): device = torch.device('cuda:0') else: device = torch.device('cpu') print(\"You have to use this script on a computer with CUDA\") exit() # SciBERT tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\") model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\").to(device) input_ids = torch.tensor(padded).to(device) attention_mask = torch.tensor(attention_mask).to(device) with torch.no_grad(): last_hidden_states = model(input_ids, attention_mask=attention_mask) # Pass tensors back to the cpu for numpy features = last_hidden_states[0][:, 0, :].cpu().numpy() Setup For this simple comparison, we will be running our example pipeline on two very different computers. NOTE: these names were invented fo this article. 1. Computer 1 \"Ol'Steevie\" - 16 inch Macbook Pro OS: macOS Big Sur Processor: 2.3 Ghz 8-Core Intel Core i9 Memory: 64 GB 2667 MHz DDR4 GPU: AMD Radeon Pro 5500M 4 GB (0 CUDA cores) 2. Computer 2 \"Clippy's Revenge\" - Henry's home built PC OS: Windows 10 Home Processor: 3.7 GHz 8-Core AMD Ryzen 7 2700x Memory: 16 GB 3000 MHz DDR4 GPU: NVIDIA GeForce RTX 2070 Super 8 GB (2560 CUDA cores) Clever readers will want to ask: why didn't you just run it once with cuda and once without on your desktop? That makes a lot of sense, and I will do that as well. However, I think this comparison of my SIH-issued ultra MacBook vs my medium-high end gaming PC is still interesting and relevant to many of us. Results Ol'Steevie: 188 seconds, 9.42 iterations / second Clippy's Revenge (CUDA): 12 seconds, 1.54 iterations / second Clippy's Revenge (CPU): 481 seconds, 24.09 iterations / second Discussion What did we learn? In this case, using CUDA was 15x faster than the MacBook, and 40x faster than using my desktop's CPU!!! It is also interesting to see how different CPU performance was between my MacBook and my desktop. It appears that the higher speed of my CPU was not sufficient to make up for the huge difference in RAM. This also suggests that performance on more typical laptops or desktops will be absolutely atrocious. If you use PyTorch already, it is super easy to make the switch to using it with CUDA. If you have access to a device with CUDA, it is a no brainer to run your deep learning pipelines on CUDA instead of CPU. Further Reading The original paper An explanation with pictures Hugging Face Transformers : lots of great pre trained models to choose from!","tags":"misc","url":"https://sydney-informatics-hub.github.io/tidbits/cuda-vs-cpu-bert-embedding-benchmarks.html","loc":"https://sydney-informatics-hub.github.io/tidbits/cuda-vs-cpu-bert-embedding-benchmarks.html"},{"title":"Show off your development history with gource","text":"Want to show off your hard work on a project? gource displays the history of a git repo with a cool moving visualisation of how the files + folders have been worked on over time. You can install it through homebrew: brew install gource . Here's the 10 year history of the OMIA project as a 1 minute video: By default, running gource in a directory with a git repo will generate a moving visualisation of the commit history - depending on the speed of development on your project there are a lot of settings you can tweak to get better results. For omia I ended up with: gource --filename-time 10 .0 --date-format \"%B %Y\" --hide filenames --time-scale 4 .0 --seconds-per-day 0 .2 --auto-skip-seconds 0 .1 Also check the project's documentation for how to export this to a video file using tools like ffmpeg .","tags":"git","url":"https://sydney-informatics-hub.github.io/tidbits/show-off-your-development-history-with-gource.html","loc":"https://sydney-informatics-hub.github.io/tidbits/show-off-your-development-history-with-gource.html"},{"title":"git pre-commit hooks: check or style your code with every commit","text":"You can use tools like black to automatically style your Python code in a consistent way. However, if you're just running it manually, you have to remember to run it regularly, and you'll probably end up with lots of git commits that are just \"Style fixes\". pre-commit uses git hooks to run different tools before each git commit. A hook that runs black allows you to automatically style any new code before it gets committed. To set up pre-commit , install the pre-commit package into your Python environment, and create a .pre-commit-config.yaml file. The config file for running black looks like: repos : - repo : https://github.com/psf/black rev : 21.6b0 hooks : - id : black Once you've created the config file you can run pre-commit install and the hooks will be set up. If you have existing code you can run pre-commit run --all-files to style it all. You can also use commit hooks for automated testing with pytest (you can run commits at each push rather than each commit if this would be too slow), fixing minor whitespace errors, or custom tests you've created. There's a big list of available hooks here to get you started.","tags":"git","url":"https://sydney-informatics-hub.github.io/tidbits/git-pre-commit-hooks-check-or-style-your-code-with-every-commit.html","loc":"https://sydney-informatics-hub.github.io/tidbits/git-pre-commit-hooks-check-or-style-your-code-with-every-commit.html"},{"title":"Getting data.table to work with the tidyverse","text":"data.table data.table is a fast, efficient data format built upon data frames/tibbles designed to work with large data sets. It comes with its own set of syntax and chaining commands which are similar to those of Python. However, if you simply can't bear to code without the pipe operator (%>%) then here's are some workaround options for you! dtplyr dtplyr is a Hadleyverse package that allows you to apply dplyr syntax to data.tables. This is done by converting a data.table into a lazy data frame in a single simple step: # Load packages library ( dtplyr ) # Turn your data . table into a lazy data frame a_lazy_df < - dtplyr :: lazy_df ( a_data_table ) Voila! Applying lazy_df() will allow you to use tidyverse verbs with your data.table. However, there is one more step required to actually view the results. This is because dtplyr saves the steps to obtain the result rather than writing out the result itself (i.e. lazy evaluation). This is so that when the lazy data frame operations are called they are executed only when required. # Apply some verbs and create a tibble a_tibble <- a_lazy_df %>% filter ( favourite_syntax == \"tidyverse\" ) %>% as_tibble () # We can also use as.data.table() or as.tibble() There is a small performance tax according to Hadley, however, it is almost on par with data.table itself. So if you need to use data.table but don't want to use its syntax, dtplyr is the package for you! For more information, check out: https://www.tidyverse.org/blog/2019/11/dtplyr-1-0-0/ tidytable tidytable allows for the use of other useful tidyverse functions, not just those from dplyr, but allowing mapping with purrr and extra tidying using tidyr. Tidytable takes the tidyverse functions and puts a dot within their name to indicate they are for use with data.table, e.g. mutate() becomes mutate.(). This makes application very simple: mtcars_dt %>% tidytable::summarise.( mean_disp = mean(disp), mean_hp = mean(hp), count = n(), .by = cyl ) %>% tidytable::arrange.(count) To look at the full list of functions, have a look at: https://markfairbanks.github.io/tidytable/reference/index.html Microbenchmarking Don't believe the above packages are speedy? Here's a comparison of the following codes using the mtcars dataset: # Create the objects required mtcars_dt <- as.data.table ( mtcars ) mtcars_lazy_dt <- lazy_dt ( mtcars_dt ) # data.table mtcars_dt [, . ( mean_disp = mean ( disp ), mean_hp = mean ( hp ), count = .N ), by = cyl ][ order ( count )] # data.frame mtcars %>% dplyr :: group_by ( cyl ) %>% dplyr :: summarise ( mean_disp = mean ( disp ), mean_hp = mean ( hp ), count = n () ) %>% dplyr :: arrange ( count ) # dtplyr mtcars_lazy_dt %>% dplyr :: group_by ( cyl ) %>% dplyr :: summarise ( mean_disp = mean ( disp ), mean_hp = mean ( hp ), count = n () ) %>% dplyr :: arrange ( count ) %>% data.table :: as.data.table () # tidytable mtcars_dt %>% tidytable :: summarise. ( mean_disp = mean ( disp ), mean_hp = mean ( hp ), count = n (), .by = cyl ) %>% tidytable :: arrange. ( count ) We can see that data.table is the fastest, as expected, and the data.frame operations are the slowest. tidytable lags a little behind dtplyr in terms of performance, but is still considerably faster than data.frame. Package Mean time (ms) data.table 0.6823924 data.frame 4.1101719 dtplyr 1.9016071 tidytable 2.5038907","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/getting-datatable-to-work-with-the-tidyverse.html","loc":"https://sydney-informatics-hub.github.io/tidbits/getting-datatable-to-work-with-the-tidyverse.html"},{"title":"A comparison of piping dataframe operations in R Tidyverse vs Python Pandas","text":"Getting the dataset Flight data downloaded from https://raw.githubusercontent.com/apache-superset/examples-data/master/tutorial_flights.csv: R library ( dplyr ) library ( readr ) data <- read_csv ( \"https://raw.githubusercontent.com/apache-superset/examples-data/master/tutorial_flights.csv\" ) > data %>% glimpse () Rows: 2 ,954 Columns: 20 $ Department <chr> \"Orange Department\" , \"Yellow Department\" , \"Yellâ€¦ $ Cost <dbl> 81.52, 343.98, 25.98, 12.99, 32.97, 61.48, 159.â€¦ $ `Travel Class` <chr> \" Economy \", \" Economy \", \" Economy \", \" Economy \", \" Ecâ€¦ $ ` Ticket Single or Return ` <chr> \"Return\" , \"Return\" , \"Return\" , \"Return\" , \"Returnâ€¦ $ Airline <chr> \" Flybe \", \" Flybe \", \" Flybe \", \" Flybe \", \" Flybe \", \" Fâ€¦ $ ` Travel Date ` <date> 2011 -02-10, 2011 -02-11, 2011 -03-02, 2011 -03-09â€¦ $ ` Origin ICAO ` <chr> \"EGPF\" , \"EGPF\" , \"EGHI\" , \"EGHI\" , \"EGPE\" , \"EGPE\" ,â€¦ $ ` Origin Name ` <chr> \"Glasgow International Airport\" , \"Glasgow Interâ€¦ $ `Origin Municipality` <chr> \" Glasgow \", \" Glasgow \", \" Southampton \", \" Southamptâ€¦ $ ` Origin Region ` <chr> \"Scotland\" , \"Scotland\" , \"England\" , \"England\" , \"â€¦ $ `Origin Country` <chr> \" United Kingdom \", \" United Kingdom \", \" United Kinâ€¦ $ ` Origin Latitude ` <dbl> 55 .8719, 55 .8719, 50 .9503, 50 .9503, 57 .5425, 57 â€¦ $ ` Origin Longitude ` <dbl> -4.433060, -4.433060, -1.356800, -1.356800, -4.â€¦ $ ` Destination ICAO ` <chr> \"EGKK\" , \"EGHI\" , \"EGPB\" , \"EPKK\" , \"EGKK\" , \"EGBB\" ,â€¦ $ ` Destination Name ` <chr> \"London Gatwick Airport\" , \"Southampton Airport\" â€¦ $ ` Destination Region ` <chr> \"England\" , \"England\" , \"Scotland\" , \"Lesser Polanâ€¦ $ `Destination Country` <chr> \" United Kingdom \", \" United Kingdom \", \" United Kinâ€¦ $ ` Destination Latitude ` <chr> \"51.148102\" , \"50.9502983093262\" , \"59.8788986206â€¦ $ `Destination Longitude` <chr> \" -0.190278 \", \" -1.35679996013641 \", \" -1.295560002â€¦ $ Distance <chr> \"595\" , \"584\" , \"993\" , \"1493\" , \"753\" , \"584\" , \"508â€¦ Python import pandas as pd data = pd . read_csv ( \"https://raw.githubusercontent.com/apache-superset/examples-data/master/tutorial_flights.csv\" ) >>> data . columns Index ([ 'Department' , 'Cost' , 'Travel Class' , 'Ticket Single or Return' , 'Airline' , 'Travel Date' , 'Origin ICAO' , 'Origin Name' , 'Origin Municipality' , 'Origin Region' , 'Origin Country' , 'Origin Latitude' , 'Origin Longitude' , 'Destination ICAO' , 'Destination Name' , 'Destination Region' , 'Destination Country' , 'Destination Latitude' , 'Destination Longitude' , 'Distance' ], dtype = 'object' ) Creating New columns from old ones R > data %>% group_by ( ` Destination Country ` ) %>% mutate ( total_cost_per_country = sum ( Cost ) , tot_cost_country_euro = total_cost_per_country * 1 .3 ) %>% glimpse () Rows: 2 ,954 Columns: 22 Groups: Destination Country [ 65 ] $ Department <chr> \"Orange Department\" , \"Yellow Department\" , \"Yelloâ€¦ $ Cost <dbl> 81.52, 343.98, 25.98, 12.99, 32.97, 61.48, 159.8â€¦ $ `Travel Class` <chr> \" Economy \", \" Economy \", \" Economy \", \" Economy \", \" Ecoâ€¦ $ ` Ticket Single or Return ` <chr> \"Return\" , \"Return\" , \"Return\" , \"Return\" , \"Return\" â€¦ $ Airline <chr> \"Flybe\" , \"Flybe\" , \"Flybe\" , \"Flybe\" , \"Flybe\" , \"Flâ€¦ $ `Travel Date` <date> 2011-02-10, 2011-02-11, 2011-03-02, 2011-03-09,â€¦ $ `Origin ICAO` <chr> \" EGPF \", \" EGPF \", \" EGHI \", \" EGHI \", \" EGPE \", \" EGPE \", â€¦ $ `Origin Name` <chr> \" Glasgow International Airport \", \" Glasgow Internâ€¦ $ ` Origin Municipality ` <chr> \"Glasgow\" , \"Glasgow\" , \"Southampton\" , \"Southamptoâ€¦ $ `Origin Region` <chr> \" Scotland \", \" Scotland \", \" England \", \" England \", \" Sâ€¦ $ ` Origin Country ` <chr> \"United Kingdom\" , \"United Kingdom\" , \"United Kingâ€¦ $ `Origin Latitude` <dbl> 55.8719, 55.8719, 50.9503, 50.9503, 57.5425, 57.â€¦ $ `Origin Longitude` <dbl> -4.433060, -4.433060, -1.356800, -1.356800, -4.0â€¦ $ `Destination ICAO` <chr> \" EGKK \", \" EGHI \", \" EGPB \", \" EPKK \", \" EGKK \", \" EGBB \", â€¦ $ `Destination Name` <chr> \" London Gatwick Airport \", \" Southampton Airport \",â€¦ $ `Destination Region` <chr> \" England \", \" England \", \" Scotland \", \" Lesser Polandâ€¦ $ ` Destination Country ` <chr> \"United Kingdom\" , \"United Kingdom\" , \"United Kingâ€¦ $ `Destination Latitude` <chr> \" 51 .148102 \", \" 50 .9502983093262 \", \" 59 .87889862060â€¦ $ ` Destination Longitude ` <chr> \"-0.190278\" , \"-1.35679996013641\" , \"-1.2955600023â€¦ $ Distance <chr> \" 595 \", \" 584 \", \" 993 \", \" 1493 \", \" 753 \", \" 584 \", \" 508 \"â€¦ $ total_cost_per_country <dbl> 376846.33, 376846.33, 376846.33, 4968.51, 376846â€¦ $ tot_cost_country_euro <dbl> 489900.229, 489900.229, 489900.229, 6459.063, 48â€¦ Python Same as above, and printing out the first 5 rows of the last 5 columns: ( data . assign ( total_cost_per_country = lambda x : ( x [[ \"Destination Country\" , \"Cost\" ]] . groupby ( \"Destination Country\" )[ \"Cost\" ] . transform ( \"sum\" ) ), tot_cost_country_euro = lambda x : x [ \"total_cost_per_country\" ] * 1.3 ) . iloc [: 5 , - 5 :] . to_markdown () ) Code output: Destination Latitude Destination Longitude Distance total_cost_per_country tot_cost_country_euro 0 51.1481 -0.190278 595 376846.33 489900.23 1 50.9503 -1.3568 584 376846.33 489900.23 2 59.8789 -1.29556 993 376846.33 489900.23 3 50.0777 19.7848 1493 4968.51 6459.06 4 51.1481 -0.190278 753 376846.33 489900.23 Data viz example A classic groupby, apply aggregation and filtering and plot stuffs R data %>% filter ( `Origin Country` == \"Germany\" ) %>% # glimpse() select ( `Origin Municipality` , Cost , Department ) %>% group_by ( `Origin Municipality` ) %>% mutate ( total_cost = sum ( Cost )) %>% ggplot ( data = . , mapping = aes ( x = `Origin Municipality` , y = total_cost , fill = Department )) + geom_bar ( stat = \"identity\" ) Output: Python ( data [ data [ \"Origin Country\" ] == \"Germany\" ] . groupby ([ \"Origin Municipality\" , \"Department\" ]) . agg ( total_cost = pd . NamedAgg ( column = \"Cost\" , aggfunc = \"sum\" ), ) . reset_index () . pivot_table ( index = [ \"Origin Municipality\" ], columns = [ \"Department\" ], values = [ \"total_cost\" ] ) . plot . bar ( stacked = True ) . get_figure () ) Output: Data pipelines A classic data pipeline with loading, processing, writing steps: R read_data <- function ( filename ) { df <- read_csv ( filename , ... ) return ( df ) } process_data <- function ( df ) { df_out <- df %>% group_by ( \"group_col\" ) %>% mutate ( new_col1 = max ( col1 ), new_col2 = sum ( col2 ) ) ... return ( df_out ) } write_data <- function ( df , outfilename ) { df_out <- df %>% select ( - col1 , - col2 ) write_csv ( df_out , outfilename ) return ( df_out ) } data <- read_data ( filename = \"path/to/input/file\" ) %>% process_data ( df = . ) %>% write_data ( df = . , outfilename = \"path/to/output/file\" ) Python def read_data ( filename : str ) -> Dataframe : df = pd . read_csv ( filename , colums = , ... ) return df def process_data ( df : Dataframe ): df_out = df df_out [[ \"new_col1\" , \"new_col2\" ]] = ( df [[ \"group_col\" , \"col1\" , \"col2\" ]] . groupby ( \"group_col\" ) . agg ({ \"col1\" : \"max\" , \"col2\" : \"sum\" , }) ) ... return df_out def write_data ( df : Dataframe , outfilename ): df_out = df . drop ([ \"col1\" , \"col2\" ], axis = 1 ) df_out . write ( outfilename ) return df_out data = ( read_data ( \"path/to/input/file\" ) . pipe ( process_data ) . pipe ( write_data , outfilename = \"path/to/output/file\" ) )","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/a-comparison-of-piping-dataframe-operations-in-r-tidyverse-vs-python-pandas.html","loc":"https://sydney-informatics-hub.github.io/tidbits/a-comparison-of-piping-dataframe-operations-in-r-tidyverse-vs-python-pandas.html"},{"title":"Set up your Jupyter environment in Docker","text":"The Problem Python lets you have multiple virtual environments and that's cool for running scripts but when you need to install Jupyter and run a notebook in Jupyter Lab (the old Jupyter Notebook or IPython notebook will be deprecated soon ), different environment might install different version of Jupyter and Lab with different plugins, ending up in a big mess. In fact there will be conflicting Jupyter configuration located under $HOME/.ipython . Proposed solution Using docker to install your containerised Jupyter enviroment seems appealing. It can be integrated in your project folder in this way: . â”œâ”€â”€ ... â”œâ”€â”€ requirements â”‚ â”œâ”€â”€ build.sh â”‚ â”œâ”€â”€ Dockerfile â”‚ â”œâ”€â”€ pip-requirements.txt â”‚ â”œâ”€â”€ run.sh â”‚ â””â”€â”€ start.sh â”œâ”€â”€ ... In which the Dockerfile looks like this: # inherit from Python image FROM python:3.8 # set your user in the container ARG NB_USER = \"myuser\" ARG NB_UID = \"1000\" ARG NB_GID = \"1000\" # install OS dependencies and perform OS operations (e.g. set your user with passwordless sudo) RUN apt-get update && \\ apt-get install -y sudo && \\ apt-get install -y python3-dev && \\ useradd -m -s /bin/bash -N -u $NB_UID $NB_USER && \\ chmod g+w /etc/passwd && \\ # give NB_USER passwordless sudo echo \" ${ NB_USER } ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers && \\ # Prevent apt-get cache from being persisted to this layer. rm -rf /var/lib/apt/lists/* # Copy requirements, install and configure the kernel. COPY --chown = \" ${ NB_UID } : ${ NB_GID } \" \"pip-requirements.txt\" \"/tmp/\" RUN pip install -r \"/tmp/pip-requirements.txt\" # set default shell, user, working directory and default command when executing the container ENV SHELL = /bin/bash USER $NB_UID WORKDIR \"/home/${NB_USER}\" CMD [ \"jupyter\" , \"lab\" , \"--no-browser\" , \"--ip=0.0.0.0\" ] Then you can build this into an image with build.sh (remember to assign executable permission to the user aka chmod u+x build.sh ): #!/bin/bash if [ -z \" $IMAGETAG \" ] ; then IMAGETAG = \"latest\" fi docker build --force-rm -t my_image: ${ IMAGETAG } . you can pass different arguments for setting your user (see build-time variables ) Then you can create your container for the first time with this script in run.sh : #!/bin/bash if [ -z \" $IMAGETAG \" ] ; then IMAGETAG = \"latest\" fi if [ -z \" $DOCKERPORT \" ] ; then DOCKERPORT = \"8888\" fi docker run -it -p 127 .0.0.1: ${ DOCKERPORT } :8888 \\ -v ` dirname ${ PWD } ` :/home/ ${ USER } /work \\ --name my_container_name my_image: ${ IMAGETAG } and subsequently run it interactively the next times using start.sh : #!/bin/bash docker start -i my_container_name The output is: and you can CTRL + click in the link to open your Jupyter Lab session in your browser. CTRL + C to stop the execution. For MAC user use the Command key instead of CTRL . Discussion Points Why we need a specific user in your image By default, all containers are run as root user, so when you mount your working folder, you might modify some files and/or create new ones with different perissions, and will not be able to modify them outside the container, unless you change them with chown command. How big is my image is going to be? A basic python 3.8 image with: pandas numpy matplotlib jupyterlab is about 1.3 GB. Jupyter Lab versions From Jupyter Lab 3.0 the installation of the plugins changed. In summary you can install extension such as interactive matplotlib without executing jupyter labextension install ... but like a normal python package pip install myextension as documented in the changelog . Python vs miniconda vs community stacks I prefer generally to install packages with pip when are for \"production\" stacks, or stacks that are sort of compartmentalised, but in general you can use the miniconda image, as a base for your Dockerfile or the community stacks . Both the latter and miniconda images are a bit bigger than standard python images and you can pull them down and use them without building your own dockerfile. Install the plugins With Jupyter Lab > 3.0 you can install widgets and plugins with pip . As example here the instructions to install the interactive matplotlib widget .","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/set-up-your-jupyter-environment-in-docker.html","loc":"https://sydney-informatics-hub.github.io/tidbits/set-up-your-jupyter-environment-in-docker.html"},{"title":"mamba: a fast replacement for conda","text":"If your conda environment is taking a long time to solve (or failing to solve altogether), you might want to try mamba , which reimplements conda in C++ with an improved dependency solving algorithm. To install in your existing conda setup (from their instructions on Github ) conda install mamba -n base -c conda-forge Then you should be able to run mamba instead of conda for commands like creating environments: mamba env create -n my_project --file environment.yml It's not a complete replacement for conda (yet), you still have to use conda activate to activate your environments, but it seems to greatly speed up installing packages and creating environments. The same developers have also created rhumba , a similar package manager for R - this might also be worth looking at for creating reproducible R environments.","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/mamba-a-fast-replacement-for-conda.html","loc":"https://sydney-informatics-hub.github.io/tidbits/mamba-a-fast-replacement-for-conda.html"},{"title":"Querying the REDCap API on Windows: avoiding 443 errors and RCurl woes","text":"One of the most attractive features of the REDCap API over other survey tools like Qualtrics, Google Forms and SurveyMonkey is the fact that the API playground supports GUI-style selection of what you want, with REDCap providing template code in a wide variety of languages. Languages include PHP, Perl, Python, R, Ruby, Java & UNIX's curl, and output format options include json, csv and XML. Further, when you request to export Records, it will nicely provide you with even more options that allow you to point and click to get the actual, real data you want, without needing to delve into the joys of XPATHs and XML. Under the hood, this is a POST form, and REDCap will explicitly show you what data you've submitted in the \"Raw request parameters\" tab. It will also provide you with the code you'd use to get the same data programmatically. There's also a \"Click the Execute Request button to execute a real API request, and it will display the API response in a text box below.\" so you can preview what you'd get back as an object when loading this into your programming language of choice. As an example, when I try to retrieve the records from our feedback form (which I've called myformname in the images/code above), it suggests the following R code for me (this returns the records themselves in a csv, and the errors in json): #!/usr/bin/env Rscript apisecret <- 'myapikey' # you get this when you enable REDCap API access for your project library ( RCurl ) result <- postForm ( uri = 'https://redcap.sydney.edu.au/api/' , token = apisecret , content = 'record' , format = 'csv' , type = 'flat' , csvDelimiter = '' , 'forms[0]' = 'myformname' , rawOrLabel = 'raw' , rawOrLabelHeaders = 'raw' , exportCheckboxLabel = 'false' , exportSurveyFields = 'true' , exportDataAccessGroups = 'false' , returnFormat = 'json' ) print ( result ) And all is well and good ... if you're on a Mac! However, when I recently tried to run this (fully working!) code on a Windows machine (since this particular survey data goes into a PowerBI dashboard I've built) - I encountered a 443 error instead! Apparently, this is a known issue , but while it's suggested to use the httr package instead (or one of the dedicated REDCap R packages), there was no template code available. After a bit of exploration, the below ended up working, and I'm sharing this template code in the hopes of saving others (at Sydney Uni and elsewhere) the hassle of having to figure this out: #!/usr/bin/env Rscript apisecret <- 'myapikey' # you get this when you enable REDCap API access for your project library ( curl ) h1 <- new_handle () handle_setform ( h1 , 'token' = apisecret , 'content' = \"record\" , 'format' = \"csv\" , 'type' = 'flat' , 'csvDelimiter' = ',' , 'forms[0]' = 'myformname' , 'rawOrLabel' = 'raw' , 'rawOrLabelHeaders' = 'raw' , 'exportCheckboxLabel' = 'false' , 'exportSurveyFields' = 'true' , 'exportDataAccessGroups' = 'false' , 'returnFormat' = 'json' ) surveyresults <- read.csv ( text = rawToChar ( curl_fetch_memory ( \"https://redcap.sydney.edu.au/api/\" , handle = h1 ) $ content ), na.strings = \"\" ) Now, the curl::handle_setform() command looks pretty similar to the RCurl::postForm() request, but it needs to be combined with the curl::curl_fetch_memory() command 1 , which has a few quirks: It returns the actual data in the content attribute, and not the data frame directly - hence the need for the $content It returns the data in raw format (and, no, setting the rawOrLabel to label does not solve this), so you need to pass it into base::rawToChar() . read.csv 's defaults are to accept a filepath, so we use an arguement called text to specify that we're straight feeding in the actual data in instead. The other useful thing to \"grab\" when working with data tends to be the data dictionary, for which the code looks quite similar: #!/usr/bin/env Rscript apisecret <- 'myapikey' # you get this when you enable REDCap API access for your project h2 <- new_handle () handle_setform ( h2 , 'token' = apisecret , 'content' = \"metadata\" , 'format' = \"csv\" , 'forms[0]' = 'myformname' , 'returnFormat' = 'csv' ) datadict <- read.csv ( text = rawToChar ( curl_fetch_memory ( \"https://redcap.sydney.edu.au/api/\" , handle = h2 ) $ content ) ) I hope this is helpful for others who use REDCap on Windows, or who need to write code that works across all of the major operating system platforms! Yes, we could have used the curl::curl_fetch_disk() command to download the file to disk, which seems to work a lot better and actually save the file as a non-binary .csv file. However, for this particular project, I'm doing a lot of data cleaning before I write the output to disk, and I'd rather not store two copies of the same scrape. â†©","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/querying-the-redcap-api-on-windows-avoiding-443-errors-and-rcurl-woes.html","loc":"https://sydney-informatics-hub.github.io/tidbits/querying-the-redcap-api-on-windows-avoiding-443-errors-and-rcurl-woes.html"},{"title":"Memory profiling in Jupyter notebooks","text":"Issues with memory use can be hard to pin down, as your program may only show issues after carrying out multiple memory intensive steps. Using the memory_profiler package in Jupyter notebooks allows you to generate a quick summary of which steps consume the most memory. First, you need to install the package through pip (or conda): pip install memory_profiler Then, in your Jupyter notebook, load it as an extension: %load_ext memory_profiler In order to profile functions, they have to be imported from a module outside the notebook. Here, I profiled a text classification model that involved loading a fairly large text vectorization model, using it to convert around 100 messages to vectors, and then running a classification model on them: def classify_messages ( messages : Sequence [ str ]) -> np . array : bert_vectorizer = BertVectorizer ( model = 'distilbert' ) classifier = load_trained_classifier () message_vectors = bert_vectorizer . make_bert_hidden_states ( messages ) # Classifier needs a dummy value for group in the input message_vectors = message_vectors . assign ( group = 0 ) predicted = classifier . predict ( message_vectors ) return predicted To profile this function, you need to call it using %mprun , specifying each individual function that you want to profile with a -f argument: % mprun - f classify_messages classify_messages ( messages ) Line # Mem usage Increment Occurences Line Contents ============================================================ 68 154.7 MiB 154.7 MiB 1 def classify_messages(messages: Sequence[str]) -> np.array: 69 680.1 MiB 525.4 MiB 1 bert_vectorizer = BertVectorizer(model='distilbert') 70 727.0 MiB 46.9 MiB 1 classifier = load_trained_classifier() 71 72 2087.8 MiB 1360.8 MiB 1 message_vectors = bert_vectorizer.make_bert_hidden_states(messages) 73 # Classifier needs a dummy value for group in the input 74 2088.1 MiB 0.3 MiB 1 message_vectors = message_vectors.assign(group=0) 75 76 2089.6 MiB 1.4 MiB 1 predicted = classifier.predict(message_vectors) 77 2089.6 MiB 0.0 MiB 1 return predicted Creating the vectors turned out to be particularly memory intensive, so I was able to reduce memory use by processing the messages in chunks: def classify_messages_chunked ( messages : Sequence [ str ], chunk_size : int = 10 ) -> np . array : bert_vectorizer = BertVectorizer ( model = 'distilbert' ) classifier = load_trained_classifier () all_preds = [] for chunk in split_into_chunks ( messages , chunk_size ): current_vectors = bert_vectorizer . make_bert_hidden_states ( chunk ) current_vectors = current_vectors . assign ( group = 0 ) predicted = classifier . predict ( current_vectors ) all_preds . append ( predicted ) result = np . concatenate ( all_preds ) return result % mprun - f classify_messages_chunked classify_messages_chunked ( messages , chunk_size = 20 ) Line # Mem usage Increment Occurences Line Contents ============================================================ 80 153.8 MiB 153.8 MiB 1 def classify_messages_chunked(messages: Sequence[str], chunk_size: int = 10) -> np.array: 88 687.5 MiB 533.7 MiB 1 bert_vectorizer = BertVectorizer(model='distilbert') 89 762.8 MiB 75.3 MiB 1 classifier = load_trained_classifier() 90 91 762.8 MiB 0.0 MiB 1 all_preds = [] 92 976.8 MiB 0.0 MiB 6 for chunk in split_into_chunks(messages, chunk_size): 93 976.8 MiB 213.5 MiB 5 current_vectors = bert_vectorizer.make_bert_hidden_states(chunk) 95 # Classifier needs a dummy value for group in the input 96 976.8 MiB 0.1 MiB 5 current_vectors = current_vectors.assign(group=0) 98 976.8 MiB 0.5 MiB 5 predicted = classifier.predict(current_vectors) 99 976.8 MiB 0.0 MiB 5 all_preds.append(predicted) 100 976.8 MiB 0.0 MiB 1 result = np.concatenate(all_preds) 101 976.8 MiB 0.0 MiB 1 return result","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/memory-profiling-in-jupyter-notebooks.html","loc":"https://sydney-informatics-hub.github.io/tidbits/memory-profiling-in-jupyter-notebooks.html"},{"title":"Setting up SSH for an easy login to VMs etc","text":"Make it easier to SSH into your VMs. You shouldn't need to enter your username $ mkdir -p ~/.ssh # make sure you have an SSH config directory $ cat >> ~/.ssh/config << EOF Host *.sydney.edu.au User <PUT-YOUR-UNIKEY-HERE> EOF This add some lines to ~/.ssh/config , that tell ssh that if you try to login to a machine whose name ends with .sydney.edu.au, it should not use your current $USER name, but should use the value inserted in <PUT-YOUR-UNIKEY-HERE> above. Now try: $ ssh research-data-int.sydney.edu.au You shouldn't need to enter a password If you do not have a private key set up on your current machine (check ls ~/.ssh/id_* for files not ending .pub ), use: $ ssh-keygen -t rsa Once you have a private key set up, you can copy your public key to the VM, which should be the last time you enter your password for that VM: $ ssh-copy-id <PUT-THE-VM-HOSTNAME-HERE> SECURITY WARNING: think carefully about putting a private key on a machine that others have sudo powers on. It's giving them the keys to wherever the corresponding public key exists. You shouldn't need to remember the VM name If you prefer an alias to writing out the full VM name, you can make another entry in ~/.ssh/config : $ cat >> ~/.ssh/config << EOF Host <ALIAS> User <PUT-YOUR-UNIKEY-HERE> HostName <PUT-THE-VM-HOSTNAME-HERE>.srv.sydney.edu.au EOF This applies only to ssh (and scp , sftp , etc.), making <ALIAS> a shorthand for login into <PUT-THE-VM-HOSTNAME-HERE>.srv.sydney.edu.au . For example, this might be a useful addition to ~/.ssh/config : Host hpc User wxyz1234 HostName hpc.sydney.edu.au Host rds User wxyz1234 HostName research-data-int.sydney.edu.au","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/setting-up-ssh-for-an-easy-login-to-vms-etc.html","loc":"https://sydney-informatics-hub.github.io/tidbits/setting-up-ssh-for-an-easy-login-to-vms-etc.html"},{"title":"Better autocompletion in Jupyter Lab","text":"Autocompletion of variable names in Jupyter Lab (or Notebook) can be frustratingly inconsistent. For a smoother development experience, the jupyterlab-lsp extension provides better completions using the same language servers as Visual Studio Code. Note that this is only available in Jupyter Lab 3+, not Jupyter Notebook. To install it in your conda environement, run: conda install -c conda-forge 'jupyterlab>=3.0.0,<4.0.0a0' jupyterlab-lsp python-language-server (or add the packages to your environment.yml file). Autocompletion To use regular Tab-completion, start typing in a code cell, hit Tab , and you should get a list of possible completions, including type information and documentation: To access documentation once the code's been written, you can hover your mouse over a function/class call and hit Ctrl : Style tips The extension also offers some features similar to Visual Studio Code or Python IDEs, like highlighting poor style or possible errors in your code (these can be disabled): (code examples from https://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb )","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/better-autocompletion-in-jupyter-lab.html","loc":"https://sydney-informatics-hub.github.io/tidbits/better-autocompletion-in-jupyter-lab.html"},{"title":"Easy delivery of local R Shiny dashboards","text":"Sometimes we don't need to deploy R Shiny dashboards to a server, and it is ok to just run them locally. In these cases, we can make it easier for clients and users to run dashboards we create for them using a nifty function: shiny::runGithub() . Here's an example of how to use this function, shamelessly borrowed from the documentation for this function: runGitHub(\"shiny_example\", \"rstudio\") # Can run an app from a subdirectory in the repo runGitHub(\"shiny_example\", \"rstudio\", subdir = \"inst/shinyapp/\") When you run this function, it will check if you have the dependencies for the dashboard installed, and automatically install them if you don't have them.","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/easy-delivery-of-local-r-shiny-dashboards.html","loc":"https://sydney-informatics-hub.github.io/tidbits/easy-delivery-of-local-r-shiny-dashboards.html"},{"title":"Annotated heatmaps in R with ComplexHeatmap","text":"Annotated heatmaps in R with ComplexHeatmap A plethora of custom heatmap packages exist for R, but the one capable of building some of the most complex, publication-ready heatmaps is called ComplexHeatmap . The power of this package lies in its ability to annotate the basic heatmap with categorical/continuous scales, or even plots (boxplots, barplots, distributions etc) to get a better sense of the data. ComplexHeatmap was initially designed with bioinformatics analyses such as gene expression in mind, but it can be used for just about anything. 1. Installation ComplexHeatmap is available on Bioconductor: if ( ! requireNamespace ( \"BiocManager\" , quietly = TRUE )) install.packages ( \"BiocManager\" ) BiocManager :: install ( \"ComplexHeatmap\" ) Or you can install it from GitHub: library ( devtools ) install_github ( \"jokergoo/ComplexHeatmap\" ) 2. Data wrangling ComplexHeatmap accepts matrices in its main argument so make sure you've converted your data to numeric matrix form. We will look at crime statistics in U.S. cities from 1970 using data from the cluster.datasets package (which, by the way, contains lots of data suitable for heatmaps that are well formatted to the point where they are almost plug-and-play). The code for wrangling this data set is simple: library ( cluster.datasets ) data ( sample.us.city.crime.1970 ) data <- as.matrix ( sample.us.city.crime.1970 ) rownames ( data ) <- sample.us.city.crime.1970 [, 1 ] data <- data [, - c ( 1 )] class ( data ) <- c ( \"numeric\" ) ComplexHeatmap does not accept data tidy format! Make sure you have all your columns and rows in matrix format where a single value is at the intersection between a row and a column value. Note that rows and columns need to be labelled accordingly as these names will be displayed as your heatmap labels. 3. The basic heatmap It's pretty easy to create a simple heatmap with your data matrix - just feed it to Heatmap() to glimpse your data. Let's also define the colours and names in this first step. # Set heatmap colours heatmap_colours <- usyd_palette ( \"ochre\" , 10 , type = c ( \"continuous\" )) # Create the heatmap Heatmap ( data , name = \"No. crimes\" , column_title = \"Types of crime\" , row_title = \"American cities\" , col = heatmap_colours ) You'll notice that dendrograms and clustering are automatically turned on. The default clustering setting is based on Euclidean distance. You can turn this off by specifying cluster_rows = FALSE and cluster_columns = FALSE . 4. Clustering We usually want to cluster our heatmap to examine patterns of similarity/dissimilarity within the data. Heatmap accepts built-in clustering arguments based on other packages as well as custom arguments. Some examples of these are: # Relies on dist() clustering_method_rows = \"pearson\" # Custom argument clustering_distance_rows = function ( x , y ) 1 - cor ( x , y ) # k-means clustering # This will automatically split your dataset (i.e. separates out the k-clustered blocks) row_km = 3 # If you have already clustered/organised your matrix separately and want your heatmap to reflect that order row_order = clustered_matrix [ 1 ,] These aren't exhaustive! Visit the ComplexHeatmap documentation to find out more (link at the bottom of the page). Remember to use set.seed each time before clustering for reproducible results. 5. Simple annotation Back to our crime data set. Let's say we've implemented some clustering and want to start annotating our heatmap. Heatmap accepts a lot of different types of annotations. Functions such as top_annotation and right_annotation can be specified either inside or outside of Heatmap depending on your preference. I've specified it outside for now. # Create annotation for the top top_annotation <- HeatmapAnnotation ( groups = anno_block ( gp = gpar ( fill = 3 : 4 ), labels = c ( \"Group 1\" , \"Group 2\" ), labels_gp = gpar ( col = \"white\" , fontsize = 10 ) ) ) # Create heatmap set.seed ( 42 ) Heatmap ( data , name = \"No. crimes\" , column_title = \"Types of crime\" , row_title = \"American cities\" , clustering_distance_rows = \"pearson\" , column_km = 2 , col = heatmap_colours , border = TRUE , top_annotation = top_annotation , rect_gp = gpar ( col = \"white\" , lwd = 0.5 )) You'll notice that anno_block is the argument for a rectangular block that can be used as a label (see below). gp and gpar define graphical parameters per annotation. 6. A more complex annotation This is where things start to get groovy. First of all, let's switch our top annotation to a boxplot using anno_box so that we can get a more informative view of data per crime type. Next, we add row annotations with rowAnnotation to Heatmap . rowAnnotation is actually the same thing as HeatmapAnnotation , except it is specifically for use with rows rather than columns. The two annotations we'll add to the side are the total instances of the proportions of crime type per city as barplots using anno_bar . We will alter some of their aesthetic parameters as well in the code below. # Create annotation for the top top_annotation <- HeatmapAnnotation ( `No. crimes committed in 1970` = anno_boxplot ( data , gp = gpar ( fill = c ( 7 : 1 )) ) ) # Create heatmap set.seed ( 42 ) heatmap <- Heatmap ( data , name = \"No. crimes\" , column_title = \"Types of crime\" , row_title = \"American cities\" , clustering_distance_rows = \"pearson\" , column_km = 2 , col = heatmap_colours , border = TRUE , top_annotation = top_annotation , rect_gp = gpar ( col = \"white\" , lwd = 0.5 ) ) + rowAnnotation ( \"Total no. crimes\" = anno_barplot ( rowSums ( data ), gp = gpar ( col = \"white\" , fill = \"#FFE200\" ), border = FALSE ), show_annotation_name = TRUE , annotation_name_rot = c ( 90 ) ) + rowAnnotation ( \"Crime proportion\" = anno_barplot ( data / rowSums ( data ) * 100 , gp = gpar ( col = \"white\" , fill = 7 : 1 ), height = unit ( 4 , \"cm\" ), border = FALSE ), show_annotation_name = TRUE , annotation_name_rot = c ( 90 ) ) # Use auto_adjust to display the row names draw ( heatmap , auto_adjust = FALSE ) It's important to note that we've saved Heatmap as an object and then pass it through draw . The auto_adjust = FALSE setting within draw makes sure that the row labels (cities in this case) are switched on, otherwise they automatically disappear. Documentation This tidbit has only touched on some of the intricacies of this package. You can really go to town with annotating your heatmap - in fact, you can also plot two heatmaps side by side, use smaller heatmaps to annotate your main heatmap, or build huge annotations based on extra data. For a complete reference of how to use ComplexHeatmap (and a lot of cool examples), refer to: https://jokergoo.github.io/ComplexHeatmap-reference/book/","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/annotated-heatmaps-in-r-with-complexheatmap.html","loc":"https://sydney-informatics-hub.github.io/tidbits/annotated-heatmaps-in-r-with-complexheatmap.html"},{"title":"Add some colour to your shell: cool stuff with themes","text":"** Introduction When working with multiple shell windows, some on a local machine and others on remote, wouldn't it be nice if you could instantly tell which was which just by looking at them? You are in luck; there are numerous options for automatic theme switching when you use ssh. Simple option: MacOS terminal options It is really easy to make your terminal change theme when you ssh. Go to your terminal preferences, then go to the profiles tab. Create a new theme called \"ssh\", and set it to whatever colours you want. Now that you have an ssh theme, create a new command (shift+command+n) and paste in ssh user1234@your.remote.server.com . Whenever you want to connect to your remote server, you can just hit shift+command+n and hit enter. This will open up a new terminal window connected to your remote server, using your ssh theme! This functionality is super easy to set up and can make it a lot easier to work with shell on local and remote at the same time. This is particularly useful when working with HPC. More complex: iTerm and theme switching via scripts You can also set up automatic theme switching within the same window with some more advanced scripting. There are many options out there on stack threads for doing this with iTerm. I stick with using the default Terminal, however if you are an iTerm user these may be an even better option!","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/add-some-colour-to-your-shell-cool-stuff-with-themes.html","loc":"https://sydney-informatics-hub.github.io/tidbits/add-some-colour-to-your-shell-cool-stuff-with-themes.html"},{"title":"VS Code Plugins: awesome tools to make your life easier","text":"Visual Studio Code Visual Studio Code is an open-source multipurpose code editor. It has many useful built in features, but becomes extremely powerful when you use plugins. I believe for many of R users it will the answer for \"what Python editor should I use?\". And you may end up using it for absolutely everything. VS Code is mostly universal Many of us end up working in many different languages, whether it is Python/R for our analysis, Java for web apps, or shell scripts for high performance computing. VS Code is smart, and knows what a huge range of languages are supposed to look like. VS Code plugins Plugins add numerous amazing features to VS Code. My favourite plugin is GitLens . This plugin provides excellent git integration, but the standout feature is the git blame functionality. When you mouseover each line, you can see the author, the age, and the commit associated with it in a tooltip. This feature is absolutely amazing, and I find myself going back and forth between RStudio and VS Code just so I can take advantage of this. There are also numerous plugins for code formatting, spell checking, and linting. These can be very useful, especially if you are working with a language you are slightly less familiar with and have to deal with yet another weird syntax and new line standard. For the Vim fanatics, there is even a Vim emulator plugin. I haven't used it, but it looks promising ( editor's note: I've used it and it does a good job of providing the core Vim functionality).","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/vs-code-plugins-awesome-tools-to-make-your-life-easier.html","loc":"https://sydney-informatics-hub.github.io/tidbits/vs-code-plugins-awesome-tools-to-make-your-life-easier.html"},{"title":"Using ellipsis (...) and purrr::pmap()","text":"Function with arbitrary arguments using ellipsis (...) Let's define a function which takes in arbitrary arguments and evaluates an expression on those arguments: eval_expres <- function ( expres , ... ){ #this part gets all the ellipsis (...) arguments, #puts them in a list, and evaluates the rest of the #function from within the environment defined by that list with ( list ( ... ), #actual guts of function eval ( parse ( text = expres )) )} Here I chose to explicitly say it needs an argument expres even though it would work with just function(...) , because this way, it will throw an error if no expression expres is given. We can test the function like so: eval_expres ( expres = \"2+3\" ) ## [1] 5 eval_expres ( expres = \"y/x\" , x = 2 , y = pi ) ## [1] 1.570796 try ( eval_expres ()) ## Error in parse(text = expres) : ## argument \"expres\" is missing, with no default Using purrr::pmap() on our function Let's make an example data frame to use with the eval_expres() function. Each row is two numbers a and b with an expression expres to evaluate. library ( tidyverse ) df = tibble ( a = c ( 1 , 2 , 3 ), b = c ( 5 , 6 , 7 ), expres = c ( \"a+b\" , \"b&#94;2\" , \"sqrt(a)\" )) We can then use a function from the purrr::pmap() family to evaluate the function multiple times, once for each row of a data frame. library ( purrr ) pmap_dbl ( .l = df , .f = eval_expres ) ## [1] 6.000000 36.000000 1.732051","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/using-ellipsis-and-purrrpmap.html","loc":"https://sydney-informatics-hub.github.io/tidbits/using-ellipsis-and-purrrpmap.html"},{"title":"Using external packages in your R package","text":"If you're building a package in R it's highly likely that you'll have to eventually rely on functions from external packages (unless you enjoy rebuilding the wheel, or can't/don't need to). This tidbit is a quick summary of my experiences with using external R packages within my own (or clients') packages. So, let's say you've set up an R package, have some code, and want to use functions from external packages to write your code. DESCRIPTION \"To depend, or not to depend, that is the question.\" The first thing you need to do is consider how heavily your code relies on external functions. You will need to state the type of dependency in the DESCRIPTION file in the root directory of your package. This will dictate whether or not external packages are installed/loaded/attached at the same time as your package is installed/loaded. Depends: if your package extensively relies on another package, it is appropriate to list that package under this category. Packages listed here will be loaded and attached with library() when your package is loaded. It's a good idea to state what version of the package is required, as you may not want just any version of these packages installed. Imports: if your package relies on a number of functions from a package throughout the code, then list it under Imports. These packages will be installed and loaded along with your package but aren't attached with library() . Adding a package to Imports is like a courtesy to your end users so that the installation is done for them. Suggests: if you use other functions/packages sporadically throughout your code - perhaps in one or two places or in a function that will not be commonly used - then pop them into Suggests. These packages will require manual installation by the end user. The difference between attaching and loading a package is that loading a package makes the package available to the R session but its functions won't be available to the global environment, whereas attaching a package means its functions are available to the global environment. Want to add a package to DESCRIPTION? No worries. Rather than adding it by hand and potentially making a mistake, use: library ( usethis ) usethis :: use_package ( \"package\" , \"dependency type\" ) NAMESPACE and specifying imports \"A function by any other name would (not) smell as sweet.\" Now that we understand the point of DESCRIPTION , let's talk about how to actually import functions and how this relates to NAMESPACE . NAMESPACE is also located in the root directory and tells us which functions are being exported or imported for a particular package. You shouldn't have to write anything into NAMESPACE manually because roxygen2 will basically handle things for you so long as you've written your function with the appropriate tags. Depends: when you're using a function from a package in Depends, you generally don't need to specify anything other than the function in your code (this is because the package is attached along with your package). I have had a handful of instances where this didn't hold true though, oddly enough, in which case you can call it via the methods for Imports (below). Imports: you should load functions from packages from these groups with @importFrom package function or using package::function . @importFrom is incredibly useful as it allows you to load specific functions rather than the entirety of a package. It will write an importFrom() line into your Namespace specifying the function/package that is loaded and accessible to the functions of your package. It is also slightly (5 us) faster than using :: . That being said, if you have a case where two functions from different packages have the same name, @importFrom won't save you, so you should use :: to explicitly state which packages the functions are from. Suggests : since these packages aren't actually installed, it is best practice to include ifrequireNamespace() within your function to alert the user that they need to install these packages before they can use your function, as below: my_fun <- function ( x ) { if ( ! requireNamespace ( \"package\" , quietly = TRUE )) { stop ( \"Package \\\"package\\\" needed for this function to work. Please install it.\" , call. = FALSE ) } } It's then good to call the external function from the package with :: to be sure it loads correctly after the package in question has been installed and loaded. Summary table \"I wasted time and now doth time waste me.\" For an even more TL;DR version of this tidbit. Category When to use? Installs with your package? Loads with your package? Function use Depends If your package uses a package A LOT Yes Loaded and attached with library() Can use as is, but if that doesn't work then use @importFrom or :: Imports For packages that are used less then A LOT but more than sporadically Yes Loaded but not attached @importFrom or :: Suggests For sporadically used packages No No :: but only after added !ifrequireNamespace","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/using-external-packages-in-your-r-package.html","loc":"https://sydney-informatics-hub.github.io/tidbits/using-external-packages-in-your-r-package.html"},{"title":"Counting zeros with dplyr summary tools","text":"The problem: losing data The dplyr package has some very useful tools for creating summaries of your data through functions like dplyr::count(), as well as the more powerful dplyr::summarise() function using the n() argument. However, by default these functions do something that is not always wanted: they don't count zeros. This is not ideal in many situations, as zero is a measurement. The solution: .drop=FALSE The solution to this problem is relatively simple. With dplyr::count(), you can find the solution in the documentation straight away: count(something) # zero is not counted, because by default .drop = TRUE count(something, .drop = FALSE) # by changing this argument, we are now counting zero","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/counting-zeros-with-dplyr-summary-tools.html","loc":"https://sydney-informatics-hub.github.io/tidbits/counting-zeros-with-dplyr-summary-tools.html"},{"title":"How to turn MS Word DOCX into JSON","text":"Turn DOCX files into JSON Do you have some Microsoft Word documents in DOCX format, but for some reason you'd like to work with JSON files? Well you are in luck! You can fairly easily convert a DOCX file into JSON. DOCX, aka Office Open XML When people think of DOCX files, they usually just imagine files that look like formatted text files. However, this processed and formatted text document is actually the result of a recipe for constructing and formatting the document that is stored within DOCX. A DOCX is actually a zipped XML file: the XML file contains all of the actual text content as well as all the information needed for formatting and structuring the documents that we look at in Microsoft Word. If you are familiar with XML and/or JSON, it should then come as no surprise that there are numerous options for converting back and forth between these two formats. Conversion workflow Converting from DOCX to JSON is simple. The Python utility simplify-docx provides this functionality through the simplify function. # Setup import docx import json from simplify_docx import simplify from collections import OrderedDict # Load source DOCX file doc_test = docx . Document ( insert_docx_file_here ) doc_test_json = simplify ( doc_test ) with open ( 'doc_test_json.json' , 'w' , encoding = 'utf-8' ) as f : json . dump ( doc_test_json , f , ensure_ascii = False , indent = 4 ) And that's it! Easy right? What does this export look like? We can loop through the objects and print it out to have a look. for key, value in doc_test_clean.items(): print(key, value) TYPE document VALUE [{'TYPE': 'body', 'VALUE': [{'TYPE': 'paragraph', 'VALUE': [{'TYPE': 'text', 'VALUE': 'JBRA Assisted Reproduction 2016;20(1):08-12 doi: 10.5935/1518-0557.20160003'}], 'style': {'indent': {'TYPE': 'CT_Ind', 'left': 127, 'right': 17, 'firstLine': 0}}}, {'TYPE': 'paragraph', 'VALUE': [{'TYPE': 'text', 'VALUE': 'Original Article'}], 'style': {'indent': {'TYPE': 'CT_Ind', 'left': 127, 'right': 0, 'firstLine': 0}}}, {'TYPE': 'paragraph', 'VALUE': [{'TYPE': 'text', 'VALUE': 'Strategies for the management of OHSS: Results from freezing-all cycles'}], That doesn't look particularly easy to read. But we can see here that we have a JSON with numerous objects that contain text information and style information. Manipulating your DOCX JSON Ok so you have a JSON now, but what if you want to remove certain parts of it? For example, what if you want to remove all tables from your document? You can loop through the objects in your JSON and make changes based on the identity of the objects. # Open your JSON doc_test_clean = json . load ( open ( \"doc_test_json.json\" ), object_pairs_hook = OrderedDict ) # Prune those tables for i in range ( len ( doc_test_clean )) : if doc_test_clean [ i ][ \"TYPE\" ] == \"table\" : doc_test_clean . pop ( i ) break # Save the output JSON with open ( 'doc_test_clean.json' , 'w' , encoding = 'utf-8' ) as f : json . dump ( doc_test_clean , f , ensure_ascii = False , indent = 4 ) Results Here is what you get once you've converted your DOCX into a JSON. We can see that our DOCX-JSON is a \"document\" built from multiple different sections (e.g. \"body\"), that are composed of numerous paragraph/style pairs. { \"TYPE\": \"document\", \"VALUE\": [ { \"TYPE\": \"body\", \"VALUE\": [ { \"TYPE\": \"paragraph\", \"VALUE\": [ { \"TYPE\": \"text\", \"VALUE\": \"JBRA Assisted Reproduction 2016;20(1):08-12 doi: 10.5935/1518-0557.20160003\" } ], \"style\": { \"indent\": { \"TYPE\": \"CT_Ind\", \"left\": 127, \"right\": 17, \"firstLine\": 0 } } }, { \"TYPE\": \"paragraph\", \"VALUE\": [ { \"TYPE\": \"text\", \"VALUE\": \"Original Article\" } ], \"style\": { \"indent\": { \"TYPE\": \"CT_Ind\", \"left\": 127, \"right\": 0, \"firstLine\": 0 } } }, { \"TYPE\": \"paragraph\", \"VALUE\": [ { \"TYPE\": \"text\", \"VALUE\": \"Strategies for the management of OHSS: Results from freezing-all cycles\" } ], \"style\": { \"indent\": { \"TYPE\": \"CT_Ind\", \"left\": 123, \"right\": 30, \"firstLine\": 0 } } } ]}]} \"Uh, why would I want to do this?\" That is a good question that I don't have a good answer for. I originally found this method when I was trying to do some cleaning of DOCX XML files. It is pretty easy to work with the XML files inside of DOCX files, but I guess there are cases where you might prefer to work with JSON files. In that case, you now know how to turn your DOCX into JSON!","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/how-to-turn-ms-word-docx-into-json.html","loc":"https://sydney-informatics-hub.github.io/tidbits/how-to-turn-ms-word-docx-into-json.html"},{"title":"tldr: A short, sweet, alternative to man pages","text":"If you want to get something done in your shell but don't want to scour through a whole man page looking for the right combination of options, tldr might have answers for you. tldr is a shell tool (installable via brew install tldr ) that gives short, human-readable examples of common shell commands, as an alternative to man pages. For example, running tldr ls gives: tldr also has specific pages for individual git commands, with examples of how to use it in specific situations. e.g. for tldr git rebase :","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/tldr-a-short-sweet-alternative-to-man-pages.html","loc":"https://sydney-informatics-hub.github.io/tidbits/tldr-a-short-sweet-alternative-to-man-pages.html"},{"title":"Cleaning a git diff","text":"Code review is easiest when the changes offered by a head branch onto a base branch are focussed on a single purpose of change. When they are not, the diff shown in GitHub can be long and hard to read, and the pull request is more susceptible to merge conflicts. Here we use \"diff\" (elsewhere \"patch\") to refer to what's seen on the Files Changed tab in GitHub. This tidbit proposes to give some hints about how you can use git to isolate the changes relevant to a specific pull request, i.e. to \"clean the diff\", using git shell commands. Case study We have been supporting the maintainers of the Hydromad R package to adopt contemporary best practices around open-source software maintenance. Pull Request #112 proposed to resolve #61 : making the pacakge conform with the CRAN requirement that package code does not use library or require . Two things made this contribution difficult to process: The contributor made changes beyond the scope of the PR, additionally resolving linter complaints of long lines, etc, by reformatting those lines. A new commit had been added to master in the meantime, performing wholesale stylistic changes across the codebase, but not identical to the changes made by the contributor. This post covers some of the questions and ideas raised there. \"Splitting one branch into multiple patches\" below is the main solution. Preface: What does the diff in GitHub show? The diff in GitHub compares each file in the HEAD commit of the head branch to each file in the base branch... But not always the latest (\"HEAD\") commit of the base branch. GitHub will firstly try to merge the latest base branch into the head branch. This will be successful if one of the following conditions can be met: * There are no new commits on the base branch (i.e. the head is built by appending commits to the latest base commit). This is called a \"fast forward merge\". * Any new commits on the base branch modify only different files, or different parts of the same file, such that Git can perform an \"automatic merge\". Note that an automatic merge can be incorrect, which is one reason that automated testing (e.g. with GitHub workflows) is invaluable. If successful, GitHub's diff will compare the head branch to the base branch through the merge commit, thus showing only those changes introduced in the head branch. If unsuccessful, GitHub's diff will compare the head branch to the nearest common ancestor commit** shared by the head branch and the base branch. This might be the point in the base branch that the contributor started building on, or it might be the latest time the contributor merged in commits from the base branch. Preface: Starting from the right point Prevention first! Before you make any changes , you can reduce the chance of merge conflicts by making sure you're building on the latest version of the base branch. Assuming the base branch is on the remote called origin in the branch called master , the following will create a new branch in your working copy based on the latest master, and check it out (i.e. get in to the branch to work on it): git fetch origin git checkout -b my-new-branch origin/master Aside: working with a fork If you're developing a pull request from a fork of some central repo, you might want to branch from that \"upstream\" master instead. Do so by adding the upstream as a remote: git add origin https://github.com/my-username/some-repo git add upstream https://github.com/central-username/some-repo git fetch upstream git checkout -b my-new-branch upstream/master # like above git push -u origin my-new-branch This branches from upstream , but pushes to origin . Preface: Making a backup of your work If you've already made changes, you shouldn't have to worry about losing your current work before trying risky Git operations. Some people use git stash for this, but I recommend just creating a commit. Commit all changes (NB: staged and unstaged changes included): git status # Check for any files you've forgotten to add git add path/to/new/files # Add any files you've forgotten to add git commit -a -m WIP # Commit all changes, but leave a message for # myself that this commit is a work-in-progress The name (SHA ref) of the new commit should be shown after committing, or you can get it from git log . Note it somewhere if you might want to refer to it later. Or, give a name to your latest commit by copying it into another branch: git branch -b new-backup-branch-name Once you've safely made a backup, you can undo your commit and keep working on your changes: git reset HEAD&#94; # Undo the commit, but leave the files changed Note that this doesn't delete the commit, it just removes it from your current branch. You can still reference the old commit using that SHA ref you noted down, or the name of that backup branch. Should I worry about cleaning up the list of commits? Below we talk about cleaning up the diff: the set of changes to files. You might also want a neat commit history, but IMO it's not usually worth neatening up the commit history shown in a pull request. Why? One neat feature of GitHub is the option to \"Squash and Merge\" a pull request onto the base branch when it is merged. This means that all of the mess of commits made along the way will be turned into a single commit. This means that you can focus on the diff and make as many messy commits as you like to get to the right place. Not committing all your changes If you've made changes in your working copy, some of which you want to commit because they're related to the pull request you are building, and some of which you don't want to commit (because they're only for your local use, or because you want to stash them for a different pull request), there are several useful tools. Most important, perhaps, is git add --patch (or git add -p ). First, some tools to check what would be committed if you did git commit at any point: git status # show an overview of what's staged to be committed and what's not git diff --cached # show lines you've already staged to be committed git diff # show lines you've not already staged to committed To add only selected parts of files, use: git add --patch # select all the lines to add git commit # when you're ready... This is an interactive process, in which you type y for chunks you want to keep, and n for chunks you don't. Try it! Press ? for other commands. If you have a new file in your repo directory that's not known to git, the following tells git about that path (adds it to the index) without staging its lines: git add -N path/to/file # repeat for each new file git add --patch # select which lines to add, from all your uncommited files git commit # when you're ready... Removing files from the commit: If you've staged something and decide you don't want to commit that (whole) file: git reset path/to/file Undoing a commit but keeping the changes This will undo the latest commit, but keep the changes to the files in your working copy, allowing you to do a patch commit or similar. git reset HEAD&#94; Again, you can follow this with git add --patch and git commit . Selecting only some commits Sometimes there are good things localised in past commits... or bad things you want to remove from a diff. NB: You might want to make a backup of your work first. You can show what was in a specific commit with: git show some-commit-ref If you want to select only some commits, you have a few options: interactive rebase: this is really powerful, but may take a bit of practice to get used to. Identify a good commit you've built on, then use git rebase -i that-good-commit-ref , and edit the file shown to pick or skip commits since then that you want. You will need to force-push any changes to GitHub git push -f . adopt only selected commits with cherry-pick : Start a clean branch git checkout -b new-branch-name that-good-commit-ref and use git cherry-pick to adopt individual commits from your changes. Make sure to do it in order. undo selected commits with git revert . Undoing changes to some files Often you're happy with the diff you've got on some files, but not with others. You can use git checkout to adopt the state of a selected file from a selected commit/branch. NB: You might want to make a backup of your work first. Let's say you've changed some file called path/to/file and committed those changes, but now want to adopt whatever is in the base branch (because the change was irrelevant to the current pull request, or because there were conflicting changes in the base which you prefer). Let's assume the base branch is called master on a remote called origin . Then you can adopt the version from origin/master with: git fetch origin # Make sure we have the latest origin/master branch git checkout origin/master path/to/file If you want to try before you buy, you can look at the version from origin/master with: git show origin/master:path/to/file or save it to some other path with: git show origin/master:path/to/file > /tmp/put-it-here-please Splitting one branch into multiple patches Back to the case study: In a complicated situation like this, we may have a long history of commits composing the current messy diff. One approach to resolving this is to create several separate branches (and corresponding pull requests) with more focussed diffs. Create a copy of the current branch where we will commit the first patch git checkout -b patch1 . Find a common place to start, the nearest common ancestor with origin's master: base=$(git merge-base HEAD origin/master) . Here we are using a bash feature to store the merge base ref in a variable called $base . View it with echo $base . Reset patch1 back to $base , keeping all changes, but not adding them. git reset $base . Add parts of files that you want git add --patch . Add any whole files that are missing from the patch with git status then git add path/to/file . Commit git commit -m \"blah blah blah\" , then push this branch if appropriate. If you look at git diff , the remainder should be changes you want in another patch commit. Let's open a new branch like patch2 at $base keeping those changes in our working copy: git checkout -b patch2 $base . Repeat from step 4. until all the changes have been committed (and pushed, and pull requests opened) in a focussed pull request.","tags":"git","url":"https://sydney-informatics-hub.github.io/tidbits/cleaning-a-git-diff.html","loc":"https://sydney-informatics-hub.github.io/tidbits/cleaning-a-git-diff.html"},{"title":"Add some colour to your shell: modern terminal tools","text":"While I'm pretty comfortable with basic shell commands, I like the convenience of features like syntax highlighting. Lately I've discovered a few replacements for common shell commands that add an extra bit of colour: bat: cat with syntax highlighting bat is a drop-in replacement for cat that adds syntax highlighting and line numbers by default - great for when you want to check the contents of a script: On Mac you can install it via Homebrew: brew install bat exa: ls with syntax highlighting On a (very) similar note, exa is a drop-in replacement for ls that adds a bit more colour. By default, it just adds some colouring for different file types: When used with options like -l it adds some colour to the structured information, e.g. the permissions: Again, install with Homebrew: brew install exa ripgrep: an easier grep I'm very bad at using grep - I can never remember whether the file or the pattern goes first. ripgrep makes grep more convenient: by default you can just give it a pattern and it will search the current folder recursively (automatically skipping hidden files and files in your .gitignore ). You get nice colourful output with the matches highlighted: brew install ripgrep Lots more I've only covered tools that I've been using regularly - there are plenty more out there if you want to improve your shell. There's a long list here , and other fun tools like asciinema which lets you record gifs/videos of your shell sessions.","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/add-some-colour-to-your-shell-modern-terminal-tools.html","loc":"https://sydney-informatics-hub.github.io/tidbits/add-some-colour-to-your-shell-modern-terminal-tools.html"},{"title":"R Profiling: Making R less aRduous","text":"Table of contents: Brief introduction to profiling, and why we might want to do it Methods to measure function speed/performance Ways to generate function profiles Visualizing and analaysing profiles with call graphs Why might we need to profile our code I'm not a computer scientist, let alone a software developer. However, I do work with code and have been envolved with developing software. Sometimes the code that we write is simple and very easy to develop and understand, works perfectly in all situations, and requires little troubleshooting. Unfortunately, that rarely occurs. Profiling can be a useful tool to deploy to help us make our code and software better. Here are a few reasons why. Long functions can - and will - become hard to understand Short functions, with few arguments and subroutines, are easy to write, read, and often run very quickly. However, to solve more complex problems we often end up having to use longer functions, or we end up calling upon complex functions within our own functions. As our code increases in length and complexity, it becomes harder for a person to read, and potentially more difficult for a computer to evaluate. This leads directly into the next reason to profile code... Clear documentation is difficult Documentation is one of the hardest things to get right, particularly so with long and complex functions or software. Many of the words we use are relatively ambiguous and only make sense within the specific context of our conversations or writing. This can cause problems when writing documentation. Object/variable names and comments may seem clear to the writer, but might not make sense to someone else outside the specific context of the writer's experiences while developing the code. Something that makes sense to the creator...might make no sense to someone else And both problems are exacerbated when we collaborate or share our code with others. Something that makes sense to you when you first wrote the code might not make much sense to future you when you are going back to fix a bug 6 months later. It is even less likely to make sense to some other person without your unique perspective and experience if they want to use, adapt, or fix your code in the future. Profiling can help us address all of these proflems. By profiling our code we can gain a better understanding of what is happening when our code is running. It can help us pinpoint weaknesses and develop better documentation, which will in turn make it easier for our code to be shared with others. What exactly is profiling? This quote from the wikipedia article on the subject provides a good summary. While it refers to the use of profiling within classic computer programming settings, all of these reasons to profile can apply to us. \"Program analysis tools are extremely important for understanding program behavior. Computer architects need such tools to evaluate how well programs will perform on new architectures. Software writers need tools to analyze their programs and identify critical sections of code. Compiler writers often use such tools to find out how well their instruction scheduling or branch prediction algorithm is performing...\" \"Profiling\" actually can refer to several different things: \"A statistical summary of the events observed\" , aka profile \"A stream of recorded events\" , aka trace This post will go over several ways to create a statistical summary of events occuring, as well as a record of these events. A simple example To start off, here's an example of a very simple function in R. This function takes two arguments. It does some maths with one of the arguments, and it creates two messages based on the two arguments and the result of the maths. Pretty simple stuff, and stuff that R can be very fast at. MyFirstFunction <- function ( x , units ){ y <- ( x * x ) * pi message ( \"A circle with a radius of \" , x , \" \" , units ) message ( \"has an area of \" , y , \" \" , units , \"&#94;2\" ) return ( y ) } MyFirstFunction ( 42 , \"cm\" ) ## A circle with a radius of 42 cm ## has an area of 5541.76944093239 cm&#94;2 ## [1] 5541.769 But what if you want to know just how fast this function is. How can we measure speed? How can to measure function speed? One of the most popular ways to measure function speed is to wrap things in system.time() . system.time ( MyFirstFunction ( 42 , \"yards\" )) ## user system elapsed ## 0.001 0.000 0.001 When you run this, you get a measure of the amount of time it took for your function to run. However, this isn't very precise, and it only runs once so you don't get a measure of average speed. Thankfully, the microbenchmark package provides a lot more functionality. The basic functionality is similar, you wrap your function in microbenchmark() and run it. By default, this will run your function 100 times and record the average time it took to evaluate in microseconds. library ( microbenchmark ) microbenchmark ( MyFirstFunction ( 42 , \"yards\" )) ## Unit: microseconds ## expr min lq mean median uq ## MyFirstFunction(42, \"yards\") 276.913 280.5095 291.9876 286.7235 291.5805 ## max neval ## 393.314 100 292 microseconds is pretty darn fast. A simple function like this probably doesn't need to be profiled. But what about a more complex function? Scenario: complex function, confusing documentation The reason I started learning about software profiling was because I joined a project to develop an R package that had already started. There were some pretty complex functions with difficult to understand functions. There were also some potential issues with performance. I wanted to scope out performance bottlenecks in the code, and also gain a better understanding of what exactly was going on so I could improve performance. get_all_images_with_exifs_metadata <- function ( images_directory_path , exifs_attributes_tags_path , output_path = NULL ) { message ( \"Gathering your images\" ) pb <- progress_bar $ new ( total = 100 ) purrr :: walk ( 1 : 100 , ~ { pb $ tick (); Sys.sleep ( 0.1 )}) images_not_having_key_attributes <- c () count_of_not_processed_images <- 0 exifs_tool_path <- Sys.which ( \"exiftool\" ) if ( is.na ( exifs_tool_path ) || exifs_tool_path == '' ) { stop ( \"exifs tool not installed. Install ExifTool from https://sno.phy.queensu.ca/~phil/exiftool/ before continue to use this package.\" , \"\\n\" ) } # Read user specified attribute tags df_exifs_attributes_tags_mapping <- read_csv ( exifs_attributes_tags_path , col_names = TRUE , col_types = cols ()) %>% clean_names () # Convert attribute tag mapping to a valid dataframe pb <- progress_bar $ new ( total = 100 ) message ( \"Looking for attribute tags\" ) for ( i in 1 : nrow ( df_exifs_attributes_tags_mapping )) { pb $ tick ( 0 ) df_exifs_attributes_tags_mapping [ i , 2 ] <- df_exifs_attributes_tags_mapping [ i , 2 ] %>% make_clean_names () } exifs_tagged_attributes <- df_exifs_attributes_tags_mapping [[ 2 ]] exifs_tagged_attributes <- unlist ( strsplit ( exifs_tagged_attributes , split = \", \" )) message ( \"Processing your images\" ) pb <- progress_bar $ new ( total = 100 ) purrr :: walk ( 1 : 100 , ~ { pb $ tick (); Sys.sleep ( 0.1 )}) df <- read_exif ( images_directory_path , tags = c ( \"FileName\" , \"CreateDate\" , \"DateTimeOriginal\" , \"FileSize\" , \"Keywords\" ), recursive = TRUE , args = NULL , quiet = TRUE ) # Clean up column names df <- df %>% separate ( FileName , c ( \"File\" , \"Extension\" ), sep = \"[.]\" ) %>% mutate ( Extension = tolower ( Extension )) df <- df %>% clean_names () # Filter only the jpg images df <- df %>% filter ( extension == \"jpg\" ) # Filter only the images which have size greater than 0 df_valid_images <- df %>% filter ( file_size > 0 ) df_invalid_images <- df %>% filter ( file_size == 0 ) # Check if the R data frame has the right keywords pb <- progress_bar $ new ( total = 100 ) purrr :: walk ( 1 : 100 , ~ { pb $ tick (); Sys.sleep ( 0.1 )}) message ( \"Checking your data\" ) df_valid_images <- get_image_with_exifs_attribute_tag_mapping ( df_valid_images , df_exifs_attributes_tags_mapping ) df_valid_images [ df_valid_images == \"NA\" ] <- NA df_valid_images [ 'species' ] <- df_valid_images [ 'species_1' ] df_valid_images [ 'count' ] <- df_valid_images [ 'no_of_species_1' ] # Creates new rows for each additional species in an image, and converts species names to lower case message ( \"Making new rows for additional species\" ) df_all_images <- get_additional_species ( df_valid_images ) pb <- progress_bar $ new ( total = 100 ) purrr :: walk ( 1 : 100 , ~ { pb $ tick (); Sys.sleep ( 0.1 )}) df_all_images <- df_all_images %>% mutate ( species = tolower ( species )) processed_count <- nrow ( df_all_images ) message ( \"Images successfully wrangled\" ) print ( processed_count ) return ( df_all_images ) } What do we want to know about this function? Here is what I wanted to find out: - What do all of the sub functions do? What order are the sub function called in? Where are the performance bottlenecks? Are any bits of code not being used? Basic R profiling: Primary data collection tools Thankfully, R studio has some very powerful built in profiling tools. You can easily use the GUI to interact with these, but you can also interact with them using code. # Standard built in R profiling tools library ( profvis ) library ( proftools ) Basic R profiling: Create a profile of your function To profile your code, you can just go up to the profiling menu tab and click \"start profiling\". However, another option is to manually start profiling with code. I've shown how to do this here. I first start profiling with Rprof() , where you can see I specified a name for the profile output to be saved. I then run the code I want to profile. Once this is done, I then stopped profiling byt clicking on the red box in R studio's GUI. Rprof ( filename = \"demo_prof.out\" , # file name for your profile append = FALSE , interval = 0.02 , line.profiling = TRUE ) # Run the function you want to profile test <- get_all_images_with_exifs_metadata ( \"/Users/hlyd4326/Desktop/Some_files\" , \"/Users/hlyd4326/Desktop/A_spreadsheet.csv\" ) # Click on the red box in the console in R studio to end profiling. Basic R profiling: Results When you stop profiling, a profile visualization is generated by the profvis package. This visualization shows two things. One shows your code line by line with measurements of memory consumption and time consumption for each line. You also get to see a flame graph, which show long long each bit of code took to run. This can be scrolled and zoomed, but can be a bit hard to understand. I personally find the line by line measurements to be the most useful bit. Here we can see that two helper functions are consuming a fair chunk of memory and are taking a bit of time to process. Basic R profiling: Flame Graph However, you can also bring back in the profile output and manually create a flame graph using a proftools function. This can be more easily read. pd <- readProfileData ( \"demo_prof.out\" ) flameGraph ( pd ) From this flame graph you can see that some portions of the code took the majority of memory/time to run, but also that other portions took a lot of time but really didn't do much. One of those actually ended up being a faulty progress bar that just pointlessly counted down time while nothing was happening. What did I learn from using R studio's default profiling tools? Certain portions of the function accounted for the bulk of memory consumption and compute time. The memory utilization did not appear to be optimized: only 12gb of 64gb available RAM were used. The inverse of this is that the function should run ok on far less powerful machines. I still had some questions about how this function was working, so I went to look for more solutions. What about some sort of call graph? Call graphs are a type of control flow diagram, and help us visualise how computer processes function. Commonly used in software development to analyse code and improve documentation. What tools exist for generating these reports in R? One possibility: visNetwork function dependency graph I came across a very promising method to makea call graph that used visNetwork 's very cool visualizations. library ( DependenciesGraphs ) deps1 <- funDependencies ( \"package:exifpro2camtrapr\" , \"get_all_images_with_exifs_metadata\" ) plot ( deps1 ) However it doesn't appear to be able to properly understand the pipe opperator. Ironically the documentation for this function wasn't very clear, so I wasn't able to figure out if there was a way to make it so that the function would understand the pipe operator. Pilgrimage to Bioconductor to find solutions After much Googling, I found a solution in the documentation of the proftools package: a profile call graph function called plotProfileCallGraph() that would create a nice flow chart of what happens when you run your function. However, it requires two packages that are not in the CRAN ecosystem. However, they are on Bioconductor # Here's how to install from Bioconductor: # install.packages(\"BiocManager\") # BiocManager::install() library ( graph ) library ( Rgraphviz ) Rgraphviz is actually an R implementation of graphviz, which is a very popular tool for making call graphs for many other programming languages. A call graph that overcomes the pipe Once I had these packages installed, I could then generate a call graph of my function. graph1 <- plotProfileCallGraph ( pd ) Interestingly, when I generated this while making this post, I realized that I had actually cloned a past development build which had already been optimized somewhat! A call graph that overcomes the pipe Here is the call graph I originally created months ago when working on this piece of software. You can see here that there are portions of the function that don't do anything, and that much of the compute time is wasted by the useless progress bars. How did call graphs help my code? They helped me recognize the order in which sub functions and helper functions were called. They helped confirm that some portions of code were useless. However, in the end the code had to be scrapped because of fundamental issues. While profiling and call graphs could help me make sense of the code, the function fundamentally was not built to the proper spec. Conclusions Profiling can help you understand what your code is doing, and when. Call graphs can help you visualize complex functions. Neither are a substitute for good coding practices. When I gave this presentation, Joel brought up this famous quote from Donald Knuth: \"premature optimization is the root of all evil (or at least most of it) in programming.\" You probably don't need to worry about profiling every function or measuring the amount of time it takes for a function to calculate the area of a circle. However, when you run into issues or know that your users may likely run into problems, profiling can help us better understand our code. It can help us identify performance bottlenecks and develop better documentation, and in many situations it can help us make our code better. Bonus: Use Case - HPC Optimizing code is particularly important when making use of shared HPC resources or using expensive cloud computing resources. However, most of the R profiling techniques I discussed today focused on profiling code that runs locally on individual machines. When developing R code for use on HPC or other distributed computing, one option is to test code on a local machine and create profiles on code using subsets of data that can run within reasonable time frames on less powerful machines. This way you can identify potential performance bottlenecks locally and hopefully implemement performance improvements before running at full scale on distributed computing. R packages # Precise speed metrics library ( microbenchmark ) # Basic profiling tools library ( profvis ) library ( proftools ) # Support for call graphs library ( DependenciesGraphs ) library ( graph ) library ( Rgraphviz ) / / \\ ___ / __ / \\ Thanks / /:/ _ / / \\ \\ \\ : \\ for / /:/ / \\ / /:/ \\__\\ : \\ coming ! / /:/ /:: \\ / __ /:: \\ ___ / /:: \\ if / __ /:/ /:/ \\ : \\ \\__\\ / \\ : \\__ / __ / \\ /:/ \\ : \\ you want the slides \\ \\ : \\ /:/~/:/ \\ \\ : \\ / \\ \\ \\ : \\ /:/ __\\ / they are going to \\ \\ ::/ /:/ \\__\\ ::/ \\ \\ ::/ be on GitHub. \\__\\ / /:/ / __ /:/ \\ \\ : \\ / __ /:/ \\__\\ / \\ \\ : \\ \\__\\ / \\__\\ /","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/r-profiling-making-r-less-arduous.html","loc":"https://sydney-informatics-hub.github.io/tidbits/r-profiling-making-r-less-arduous.html"},{"title":"R 4.0 and Mac OS Issues - fixing data.table and fst","text":"The R maintainers have made some changes to how R is built on Mac OS for R 4.0+: this should make some packages like Stan easier to use, however it also seems to cause issues with packages like data.table and fst that use multi-threading. When loading these packages you may see a warning about how they're only using single threads. After checking multiple setup guides (e.g. here and here ), I was able to fix these issues using the following steps: Install the XCode command line tools: xcode-select --install Install llvm , gcc and libomp through Homebrew: brew install llvm gcc libomp Add the following to ~/.R/Makevars : XCBASE := $( shell xcrun --show-sdk-path ) LLVMBASE := $( shell brew --prefix llvm ) GCCBASE := $( shell brew --prefix gcc ) GETTEXT := $( shell brew --prefix gettext ) CC = $( LLVMBASE ) /bin/clang -fopenmp CXX = $( LLVMBASE ) /bin/clang++ -fopenmp CXX11 = $( LLVMBASE ) /bin/clang++ CXX14 = $( LLVMBASE ) /bin/clang++ CXX17 = $( LLVMBASE ) /bin/clang++ CXX1X = $( LLVMBASE ) /bin/clang++ CFLAGS = -g -O3 -Wall -pedantic -std = gnu99 -mtune = native -pipe CXXFLAGS = -g -O3 -Wall -pedantic -std = c++11 -mtune = native -pipe CPPFLAGS = -isystem \" $( LLVMBASE ) /include\" -isysroot \" $( XCBASE ) \" LDFLAGS = -L \" $( LLVMBASE ) /lib\" -L \" $( GETTEXT ) /lib\" --sysroot = \" $( XCBASE ) \" SHLIB_OPENMP_CFLAGS = -fopenmp SHLIB_OPENMP_CXXFLAGS = -fopenmp SHLIB_OPENMP_FCFLAGS = -fopenmp SHLIB_OPENMP_FFLAGS = -fopenmp FC = $( GCCBASE ) /bin/gfortran F77 = $( GCCBASE ) /bin/gfortran FLIBS = -L $( GCCBASE ) /lib/gcc/9/ -lm After this, you should be able to successfully compile packages like data.table with multi-threading support: install.packages ( \"data.table\" , type = \"source\" ) If your setup is working, loading data.table (or fst ) should print a message about how multiple threads are being used: data.table 1.12.8 using 8 threads (see ?getDTthreads).","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/r-40-and-mac-os-issues-fixing-datatable-and-fst.html","loc":"https://sydney-informatics-hub.github.io/tidbits/r-40-and-mac-os-issues-fixing-datatable-and-fst.html"},{"title":"Generating an automatic index for Github Pages","text":"The output you deliver to clients might include multiple reports, e.g. HTML files generated from RMarkdown. In order to display these using Github Pages, it's useful to create an index that links to them, so clients can easily access all reports via a landing page. I found this Python script on Stack Overflow (written by user Matthew Brett) which scans a directory (e.g. the docs/ directory that Github Pages uses), and generates the index: \"\"\" Build index from directory listing make_index.py </path/to/directory> [--header <header text>] \"\"\" INDEX_TEMPLATE = r \"\"\" <html> <body> <h2>$ {header} </h2> <p> % f or name in names: <li><a href=\"$ {name} \">$ {name} </a></li> % e ndfor </p> </body> </html> \"\"\" EXCLUDED = [ 'index.html' ] import os import argparse # May need to do \"pip install mako\" from mako.template import Template def main (): parser = argparse . ArgumentParser () parser . add_argument ( \"directory\" ) parser . add_argument ( \"--header\" ) args = parser . parse_args () fnames = [ fname for fname in sorted ( os . listdir ( args . directory )) if fname not in EXCLUDED ] header = ( args . header if args . header else os . path . basename ( args . directory )) print ( Template ( INDEX_TEMPLATE ) . render ( names = fnames , header = header )) if __name__ == '__main__' : main () If you have Github Pages set up to use the docs/ directory, then you should be able to generate an index for your project with: make_index.py docs/ --header \"My Project Name\" > docs/index.html","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/generating-an-automatic-index-for-github-pages.html","loc":"https://sydney-informatics-hub.github.io/tidbits/generating-an-automatic-index-for-github-pages.html"},{"title":"Getting ggplot to start at 0,0","text":"This is not really a tech tidbit in itself, but is something that would be helpful for many of us when we need it, so I'm reposting/adapting it from StackOverflow / the interwebs. In ggplot, in order to force the origin of the plot to start at (0,0) and not be padded, you need to add the following options: scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) Default behavior results in mpg %>% ggplot(aes(x = hwy-12, y = cty)) + geom_point() Adding the above options results in things starting at (0,0), but you lose some data (as in this SO post) You also have to set the limits options to ensure all of the data is displayed! mpg %>% ggplot(aes(x = hwy-12, y = cty)) + geom_point() + theme_bw() + scale_x_continuous(limits = c(0,40), expand = c(0, 0)) + scale_y_continuous(limits = c(0,40), expand = c(0, 0)) Finally, if you don't want to manually specify the limits you can use the below: mpg %>% ggplot(aes(x = hwy-12, y = cty)) + geom_point() + theme_bw() + scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.1))) + scale_x_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.1)))","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/getting-ggplot-to-start-at-00.html","loc":"https://sydney-informatics-hub.github.io/tidbits/getting-ggplot-to-start-at-00.html"},{"title":"Formatting Code in Emails and Presentations","text":"The Annoying Bit Every time I'm trying to communicate about a piece of code in my Outlook email, I can't formatting it using ` or ```prog lang my code in here ``` It would be great using such shortcuts in Outlook (at least the Outlook web app does not have some code formatting option), like in Microsoft Teams and Slack. The Solution Unfortunately the only way is to use this website https://tohtml.com/ and copy paste your bit that you want to format. Can handle many languages. The result is this nice email or Powerpoint slide.","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/formatting-code-in-emails-and-presentations.html","loc":"https://sydney-informatics-hub.github.io/tidbits/formatting-code-in-emails-and-presentations.html"},{"title":"Pandas dataframe to Markdown","text":"The Problem In building up code and tracking the bugs, it is extremely useful posting the traceback in a GitHub issue. The problem is that when we are dealing with pandas dataframes, it is not easy to convert the df into a markdown. Given the following df: import pandas as pd df = pd . DataFrame ([[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ]], columns = [ 'letters' , 'numbers' ]) You need to incorporate it into something like this: ```python letters numbers 0 a 1 1 b 2 2 c 3 ``` to have the following output: letters numbers 0 a 1 1 b 2 2 c 3 The solution Starting from version 1.0 Pandas now inlude a to_markdown() method that would make the things easier for us. See below: >> print ( df . to_markdown ()) | | letters | numbers | |--- : | : ----------|---------- : | | 0 | a | 1 | | 1 | b | 2 | | 2 | c | 3 | Then copy and paste it in a GitHub Issue:","tags":"Python","url":"https://sydney-informatics-hub.github.io/tidbits/pandas-dataframe-to-markdown.html","loc":"https://sydney-informatics-hub.github.io/tidbits/pandas-dataframe-to-markdown.html"},{"title":"Some formulas to make working in Excel easier","text":"If you're used to programming in R or Python and suddenly have to do everything in Excel due to client requirements or something, you may repeatedly find yourself wondering \"How do I do this thing I always do? Shouldn't it be easy?\". Well the answer likely is no, it isn't as easy as it should be, but yes, there is a way. Checking a cell for the presence of one of multiple partial text string matches Say the data has been entered really non-uniformly. Sometimes it says 'Yes', sometimes it's a 1 etc. like in the following table: Yes value No value Missing data Yes No n/a Yes (unneccesary details) No (unneccesary details) - 1 0 I think so Not this time NA Any cell can contain one of these options and the unnecesary details might change. So really you want to search for a list of items and if any appear, mark it as true. Example code to search any of the 'Yes' values and recode them as TRUE : = OR( NOT( ISERROR( SEARCH({ \"Yes\" , \"1\" , \"I think so\" } , cell_to_search ) ) ) ) How does it work? This is feeding a cell array of possible yes values { \"Yes\" , \"1\" , \"I think so\" } into the search function along with the cell to search. This will output a cell array which contains a 1 where it matched the search string, and a #VALUE error where it didn't, for example if the cell contents was \"Yes blah\", it will return {1, #VALUE, #VALUE} The ISERROR() function will turn this into true/false values e.g. {FALSE, TRUE, TRUE} The NOT() function negates it to {TRUE, FALSE, FALSE} The OR() function concatenates it appropriately to a single TRUE value output. Finding the unique elements in an array If you want to find the unique elements in a column and return them sorted, it's easy. Just use =SORT(UNIQUE(cells)) . However if you want to do this for values occuring across multiple columns in an array, it's annoying. First, select your array and give it a name, e.g. MyArray . Then, use this horrible formula: =SORT(UNIQUE(INDEX(MyArray,1+INT((ROW(INDIRECT(\"1:\"&COLUMNS(MyArray)*ROWS(MyArray)))-1)/COLUMNS(MyArray)),MOD(ROW(INDIRECT(\"1:\"&COLUMNS(MyArray)*ROWS(MyArray)))-1+COLUMNS(MyArray),COLUMNS(MyArray))+1))) How does it work? Lets break it into parts: Array element iterator ROW(INDIRECT(\"1:\"&COLUMNS(MyArray)*ROWS(MyArray))) . Using the formula =ROW(INDIRECT(\"1:10\")) will give me a cell array of {1;2;3;4;5;6;7;8;9;10} . So to get an iterator to go through all the elements in the array, I use ROW(INDIRECT(\"1:\"&COLUMNS(MyArray)*ROWS(MyArray))) . Let's call this iterator i now. Indexing the array INDEX(array, row, column) will pull out an element of the array. so we do appropriate modulo arithmatic on the iterator i to get the column number - MOD(i-1+COLUMNS(MyArray),COLUMNS(MyArray))+1 , and take the floor of a division (using the INT() function) to get the row number as row = 1+INT((i-1)/COLUMNS(MyArray)) . So we get INDEX(MyArray,1+INT((i-1)/COLUMNS(MyArray)),MOD(i-1+COLUMNS(MyArray),COLUMNS(MyArray))+1) Now we have a vertical array/column (let's call it col ) containing all the data which was in the array. We just have to apply SORT(UNIQUE(col)) to get out only the unique elements, and sorted in alphabetical order. Excel gotchas Referencing the contents of a cell will return the contents of the cell, unless it is blank. A blank cell will turn into a zero. Thus missing data can magically turn into non-missing data in the middle of your analysis. Instead of =cell_ref use =if(isblank(cell_ref),\"\",cell_ref) to keep blanks blank. Checking if a cell is blank using isblank(cell_ref) will return false if the cell contains a formula which returns a blank string as a value. In this case use cell_ref=\"\" instead. Examples In the file excel_formulas_example.xlsx are examples of the things described above.","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/some-formulas-to-make-working-in-excel-easier.html","loc":"https://sydney-informatics-hub.github.io/tidbits/some-formulas-to-make-working-in-excel-easier.html"},{"title":"Deploy R Shiny App on Remote Server","text":"The Problem Data Scientists are used to running R code with the click of a mouse using Rstudio and deploying it to a Shiny Server in the same way. The reality is that many production servers use command line to run the process/webserver/app/... and use a master process (the supervisor) to control the execution of the app. A Simple Solution All you need to run the app is to execute the following lines: $ R -e \"shiny::runApp()\" Note : the assumption here is that you are in a folder where an app.R is present (and where all your app code reside)! Even better, you can improve your app by creating a runapp.R file, that look like this: library ( optparse ) # declare script input options args_list <- list ( make_option ( c ( '-d' , '--data-dir' ), type = 'character' , default = NULL , help = 'Data folder path' , dest = 'data_dir' ), make_option ( c ( '-u' , '--username' ), type = 'character' , default = NULL , help = 'API username' , dest = 'username' ), make_option ( c ( '-p' , '--password' ), type = 'character' , default = NULL , help = 'API password' , dest = 'password' ) ) # parsing arguments opt_parser <- OptionParser ( option_list = args_list ); opt <- parse_args ( opt_parser ); if ( is.null ( opt $ username ) | is.null ( opt $ password )) { print_help ( opt_parser ) stop ( 'Must supply username and password' , call. = FALSE ) } username <- opt $ username password <- opt $ password shiny :: runApp ( host = '127.0.0.1' , port = 7744 , launch.browser = FALSE ) As you can see you can pass in the command line, arguments such as username and password, that can be used somewhere. Also you can add checks on those arguments. In the runApp command you can also specify the IP address and the port. Until now nothing new... you can run this from a shell in your machine. Well in a server is not that different, you still need to run that command above, but the \"Supervisor\" program will take care of that. Usually Unix machines use the systemd as processes supervisor. It controls all the Linux OS processes, such as network and others. In order to let the systemd take care of your process (in this case the shiny app), you need to create a unit file (e.g. myapp.service ), like this: [ Unit ] Description = My Shiny App After = syslog.target network.target [ Service ] Type = simple ExecStart = /bin/R /path/to/my/app/runapp.R Restart = always StandardOutput = syslog StandardError = syslog SyslogIdentifier = MyShinyApp [ Install ] WantedBy = multi-user.target And save in /etc/systemd . Then update the list of services systemctl daemon-reload , enable your service at startup systemctl enable myapp.service , and finally start your service systemctl start myapp.service . Add a syslog config file to re-direct all logs of your process/App to a specific file. Create a new configuration file in /etc/rsyslog.d/my_shiny_app.conf with the following: if $programname == 'MyShinyApp' then /path/to/log/file.log & stop Make sure permissions are all set for the specific logging file. Some More Background People on the internet mention a few ways to deploy Shiny Apps on a server (see links in section below): Shiny (R)Server Open Source - FREE Shiny Server Pro / R Studio Connect and Shinyapps.io - PAID! ShinyProxy : open source enterprise level Shiny App server, with built-in functionality for LDAP authentication and no limits on concurrent usage. - FREE Custom architectures - FREE/PAID My Conclusions/Gothas The approach above is equivalent to \"Click-Upload-Run\" your Shiny App on an open-source R Studio server. Both run a single R process! The approach above is more than enough to serve the app to a limited amount of users (< 50/100) Need to look into scaling up if the number of users is higher (ShinyProxy or R server pro/shinyconnect, etc.) Useful Links and More Reading Shiny Deployment HowTo put your Shiny App behind a proxy/load balancer: https://support.rstudio.com/hc/en-us/articles/213733868-Running-Shiny-Server-with-a-Proxy ShinyProxy 4(5) ways to run Shiny App: Shiny in Production slides : talking about async, app profiling, etc. Create an App package with Golem : good app cookie-cutter template generator How to Build a Shiny App from scratch Shiny App profiling (R code profiling) Shiny in Production workshop Async Shiny App: Cranwhales Links on Unit Files: example ProteHome example Digifarm","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/deploy-r-shiny-app-on-remote-server.html","loc":"https://sydney-informatics-hub.github.io/tidbits/deploy-r-shiny-app-on-remote-server.html"},{"title":"Bayesian Changepoint detection","text":"This mcp library is really cool! Makes bayesian changepoint detection in R really easy. Check it out with examples at https://lindeloev.github.io/mcp/","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/bayesian-changepoint-detection.html","loc":"https://sydney-informatics-hub.github.io/tidbits/bayesian-changepoint-detection.html"},{"title":"Useful Mac OS tips","text":"Some favourite keyboard shortcuts: Switch tabs: âŒ˜-shift-[ and âŒ˜-shift-] ; Close tab: âŒ˜-w Open Spotlight to search for things: âŒ˜-space Once you've found something in Spotlight, use âŒ˜-click to open its containing folder Screen capture âŒ˜-ctrl-shift-4 : allows you to select a rectangle, which will then be available in clipboard (paste straight into email or save by opening Preview and âŒ˜-n ) Switch Application: âŒ˜-tab (hold âŒ˜ to select) Switch Window within application: âŒ˜-~ Page up/down/home/end: Fn-up/down/left/right Command-line tips Use open to open an app, or to open a file Use pbcopy to dump the output of a command to the clipboard Use pbpaste to dump the clipboard to standard output (for piping into a program or a file) Drag files into the command-line to get their path (see below) Some other neat things: An icon in the status bar like or can be dragged (yes, even a file you have open in word). Drag it into your Terminal app to get the path to that file. Drag it into your email to attach it. Drag it into your web browser to upload it (in some contexts). This one's pretty cool: drag it into a Save As or Open dialog (where you get to select a file) to go to the file's directory! You may want to use âŒ˜-tab and âŒ˜-~ to navigate to the destination window while dragging. The icon next to the URL in your browser â€“ such as the padlock here â€“ can also be dragged, so you can insert the web address in your terminal or email. Install ITerm2 and install its shell integration. Among other things it gives you a status bar showing you what processes are running in that shell, the git branch / status, etc. Is Time Machine backup taking too long? Open terminal and enter sudo sysctl debug.lowpri\\_throttle_enabled=0 to speed up the process.","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/useful-mac-os-tips.html","loc":"https://sydney-informatics-hub.github.io/tidbits/useful-mac-os-tips.html"},{"title":"Adobe Acrobat Pro DC for PDF text extraction","text":"While there are many options for extracting text from PDFs using R and Python, at the end of the day Adobe rightly knows best about how to extract text from PDFs. You can very easily do this with individual pdf files using the Adobe Acrobat GUI. In order to do this automatically on batches of files, you used to be able to call on Acrobat functions using the shell/terminal. Unfortunately it seems that Adobe has closed off much of this functionality (and in fact it is now agains the terms of service). Thankfully all is not lost: there is an Adobe approved way to batch process pdfs: Action Wizard . Action Wizard sounds like something out of Dungeons and Dragons, but it is actually a somewhat difficult to find tool within Adobe Acrobat for performing a wide range of processes. Finding and using the Action Wizard By default, you likely will not see the action wizard. Here is how to find it: Open Adobe Acrobat Pro DC. You should see the option to select the \"Tools\" tab. Click on that. You should now see tab with a bunch of different tools. Scroll down, or type \"action wizard\" into the search bar to find the wizard. Add the wizard to your tools. Now that you have added the wizard to your tools, click on it and check out all of the options that you have. Import options to note: Manage actions lets youâ€¦manage the actions that you have available. You can use this to import, enable, edit, and export actions. The more actions button takes to Adobe's action exchange website. Action Wizard Plugins Action Wizard has support for building your own plugins, as well as importing plugins that have been made by others. You can view available plugins on Adobe's action Exchange: https://acrobatusers.com/actions-exchange/index.html PDF to Word Lo and behold, I found a plugin on the actions exchange that lets you batch process and save PDF files as Word files. After downloading the action, you need to import it. This is pretty easy to figure out by clicking on manage actions. Here's an image of what to do just in case. When run on a folder containing subfolders, the plugin functions recursively and will read and convert all pdfs contained within the sub folders. I ran this plugin on a set of ~900 PDFs of scientific papers, and it took ~50 minutes to process these into .docx. When I opened up one of these in Word, I saw that Acrobat has managed to perfectly replicate the PDF's format in Wordâ€¦which is not particularly useful. I got started writing a Visual Basic macro for saving as .txt when I stopped and thought: why not try to customize the plugin itself! PDF to whatever you want Thankfully, it is very easy to edit plugins in Action Wizard. To do this, go back to Manage Actions, select the action you want to edit, and then click edit. There are heaps of options here; for now we are just interested in adjusting the options for the file saving step of this action. After clicking save, the plugin is now primed to save PDF files as whatever format you want. From experimenting with .txt and .rtf files, I found that .rtf does the best job at converting files with multiple columns into one sensible column of text. When this is running, it takes in between 1 and 3 seconds to process each PDF into .rtf. Conclusion Need to do something with PDF files? Actions in Adobe Acrobat may end up being the best option for you. There are loads of pre-built actions on the actions exchange, and Adobe has loads of built in options for building or editing actions. You can also write actions in javascript if you feel like it!","tags":"Misc","url":"https://sydney-informatics-hub.github.io/tidbits/adobe-acrobat-pro-dc-for-pdf-text-extraction.html","loc":"https://sydney-informatics-hub.github.io/tidbits/adobe-acrobat-pro-dc-for-pdf-text-extraction.html"},{"title":"fst: A very fast file format for R","text":"The fst package allows very fast reading and writing of data from R - much faster than .rds files. This can save a lot of time when dealing with medium to large datasets. By default it uses less compression than .rds , but even with compress = 100 , it produces similar file sizes to rds and is still much faster. It can also read arbitrary rows and columns from a file, so you can: Save time by only reading in the columns you need from a file Save memory by only reading one chunk from the file at a time and processing it (you do need a processing task that can be done separately on each chunk) A simple example of processing a file in chunks is below: you could replace the process_chunk() function with any function that takes a dataframe as input: library ( fst ) df <- data.frame ( id = 1 : 1000 , event = sample ( c ( 0 , 1 ), 1000 , replace = TRUE , prob = c ( 0.88 , 0.12 ))) # compress = 100 for more compression, slightly slower reads fst :: write_fst ( df , \"df.fst\" , compress = 50 ) process_chunk <- function ( .data ) { sum ( .data $ event ) } # Get info (e.g. number of rows) without reading the # full file file_info <- fst :: metadata_fst ( \"df.fst\" ) total_rows <- file_info $ nrOfRows chunk_starts <- seq ( 1 , total_rows , by = 100 ) event_counts <- purrr :: map_dbl ( chunk_starts , function ( start_row ) { # Don't go past the last row end_row <- min ( start_row + 99 , total_rows ) chunk <- fst :: read_fst ( \"df.fst\" , from = start_row , to = end_row ) process_chunk ( chunk ) }) event_counts #> [1] 16 10 7 12 15 11 9 12 13 18 # Final result: combine the chunk results, e.g. # with bind_rows() sum ( event_counts ) #> [1] 123 The package plans to add an append option when writing files - once this becomes available you will also be able to write to a .fst file in chunks.","tags":"R","url":"https://sydney-informatics-hub.github.io/tidbits/fst-a-very-fast-file-format-for-r.html","loc":"https://sydney-informatics-hub.github.io/tidbits/fst-a-very-fast-file-format-for-r.html"}]};